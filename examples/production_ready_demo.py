#!/usr/bin/env python3
"""
Production Ready Mock Spark Demo

This example showcases Mock Spark's production-ready capabilities with:
- 515 tests passing (100% pass rate)
- 100% Zero Raw SQL architecture
- Database-agnostic design (DuckDB/PostgreSQL/MySQL/SQLite)
- 100% PySpark compatibility (PySpark 3.2)
- Enterprise-grade features
- Version 2.0.0 with pure SQLAlchemy stack

Run this file to see Mock Spark's full capabilities in action.
"""

from sparkless.sql import (
    SparkSession,
    functions as F,
    Window,
)
from sparkless.sql.types import (
    StringType,
    IntegerType,
    DoubleType,
    BooleanType,
    DateType,
    TimestampType,
    DecimalType,
    ArrayType,
    MapType,
    BinaryType,
    NullType,
    FloatType,
    ShortType,
    ByteType,
    StructType,
    StructField,
)
from sparkless.error_simulation import MockErrorSimulator
from sparkless.performance_simulation import MockPerformanceSimulator
from sparkless.data_generation import MockDataGenerator
from sparkless.core.exceptions import AnalysisException
import time


def print_section(title: str) -> None:
    """Print a formatted section header."""
    print(f"\n{'=' * 60}")
    print(f"ðŸŽ¯ {title}")
    print("=" * 60)


def print_subsection(title: str) -> None:
    """Print a formatted subsection header."""
    print(f"\nðŸ“‹ {title}")
    print("-" * 40)


def main() -> None:
    """Demonstrate Mock Spark's production-ready capabilities."""
    print("ðŸš€ Mock Spark Production Ready Demo")
    print("=" * 60)
    print("âœ… 515 tests passing (100% pass rate)")
    print("âœ… 100% Zero Raw SQL architecture")
    print("âœ… Database-agnostic (DuckDB/PostgreSQL/MySQL/SQLite)")
    print("âœ… 100% PySpark compatibility")
    print("âœ… Enterprise-grade features - Version 2.0.0")
    print("=" * 60)

    # Create Mock Spark Session
    print_section("Creating Mock Spark Session")
    spark = SparkSession("ProductionDemo")
    print(f"âœ“ Created session: {spark.app_name}")
    print(f"âœ“ Version: {spark.version}")

    # Demonstrate comprehensive data types
    print_section("Comprehensive Data Types Support")

    # Create complex schema with all supported data types
    complex_schema = StructType(
        [
            StructField("id", IntegerType()),
            StructField("name", StringType()),
            StructField("age", IntegerType()),
            StructField("salary", DoubleType()),
            StructField("is_active", BooleanType()),
            StructField("birth_date", DateType()),
            StructField("created_at", TimestampType()),
            StructField("balance", DecimalType(10, 2)),
            StructField("tags", ArrayType(elementType=StringType())),
            StructField("metadata", MapType(StringType(), StringType())),
            StructField("avatar", BinaryType()),
            StructField("optional_field", NullType()),
            StructField("score", FloatType()),
            StructField("category_id", ShortType()),
            StructField("status_code", ByteType()),
        ]
    )

    print("âœ“ Created complex schema with 15+ data types")
    print(f"âœ“ Schema fields: {len(complex_schema.fields)}")

    # Generate realistic test data
    print_subsection("Data Generation")
    sample_data = MockDataGenerator.create_realistic_data(
        complex_schema, num_rows=100, seed=42
    )
    print(f"âœ“ Generated {len(sample_data)} realistic records")

    # Create DataFrame with complex schema
    df = spark.createDataFrame(sample_data, complex_schema)
    print(f"âœ“ Created DataFrame with {df.count()} rows")
    print(f"âœ“ DataFrame columns: {df.columns}")

    # Demonstrate advanced DataFrame operations
    print_section("Advanced DataFrame Operations")

    # Complex filtering and selection
    print_subsection("Complex Filtering and Selection")
    filtered_df = df.filter(
        (F.col("age") > 20) & (F.col("is_active")) & (F.col("salary") > 50000)
    ).select(
        F.col("id"),
        F.col("name"),
        F.col("age"),
        F.col("salary"),
        F.upper(F.col("name")).alias("upper_name"),
        F.length(F.col("name")).alias("name_length"),
        F.when(F.col("age") > 30, F.lit("Senior"))
        .when(F.col("age") > 25, F.lit("Mid-level"))
        .otherwise(F.lit("Junior"))
        .alias("level"),
        F.coalesce(F.col("salary"), F.lit(0.0)).alias("safe_salary"),
    )

    print("âœ“ Complex filtering applied")
    print(f"âœ“ Filtered rows: {filtered_df.count()}")

    # Window functions demonstration
    print_subsection("Window Functions")
    window_spec = Window.partitionBy("level").orderBy(F.desc("salary"))

    windowed_df = (
        filtered_df.withColumn("row_num", F.row_number().over(window_spec))
        .withColumn("rank", F.rank().over(window_spec))
        .withColumn("dense_rank", F.dense_rank().over(window_spec))
        .withColumn("avg_salary_by_level", F.avg("salary").over(window_spec))
        .withColumn("prev_salary", F.lag("salary", 1).over(window_spec))
        .withColumn("next_salary", F.lead("salary", 1).over(window_spec))
    )

    print("âœ“ Window functions applied")
    print(f"âœ“ Windowed DataFrame rows: {windowed_df.count()}")

    # Aggregation operations
    print_subsection("Advanced Aggregations")
    agg_df = (
        windowed_df.groupBy("level")
        .agg(
            F.count("*").alias("count"),
            F.avg("salary").alias("avg_salary"),
            F.max("salary").alias("max_salary"),
            F.min("salary").alias("min_salary"),
            F.sum("salary").alias("total_salary"),
            F.collect_list("name").alias("names"),
        )
        .orderBy(F.desc("avg_salary"))
    )

    print("âœ“ Advanced aggregations completed")
    agg_df.show()

    # String operations
    print_subsection("String Operations")
    string_df = df.select(
        F.col("name"),
        F.upper(F.col("name")).alias("upper"),
        F.lower(F.col("name")).alias("lower"),
        F.length(F.col("name")).alias("length"),
        F.trim(F.col("name")).alias("trimmed"),
        F.regexp_replace(F.col("name"), "a", "X").alias("replaced"),
        F.split(F.col("name"), " ").alias("words"),
    ).limit(5)

    print("âœ“ String operations completed")
    string_df.show()

    # Mathematical operations
    print_subsection("Mathematical Operations")
    math_df = df.select(
        F.col("salary"),
        F.abs(F.col("salary")).alias("abs_salary"),
        F.round(F.col("salary"), 2).alias("rounded_salary"),
        F.ceil(F.col("salary") / 1000).alias("salary_k_ceil"),
        F.floor(F.col("salary") / 1000).alias("salary_k_floor"),
        F.sqrt(F.col("salary")).alias("salary_sqrt"),
    ).limit(5)

    print("âœ“ Mathematical operations completed")
    math_df.show()

    # Date/time operations
    print_subsection("Date/Time Operations")
    datetime_df = df.select(
        F.col("created_at"),
        F.current_timestamp().alias("now"),
        F.current_date().alias("today"),
        F.year(F.col("created_at")).alias("year"),
        F.month(F.col("created_at")).alias("month"),
        F.day(F.col("created_at")).alias("day"),
    ).limit(5)

    print("âœ“ Date/time operations completed")
    datetime_df.show()

    # Error simulation demonstration
    print_section("Error Simulation Framework")

    error_sim = MockErrorSimulator(spark)

    # Add error rules
    error_sim.add_rule(
        "table",
        lambda name: "error" in name.lower(),
        AnalysisException("Simulated table error"),
    )

    error_sim.add_rule(
        "sql",
        lambda query: "ERROR" in query.upper(),
        AnalysisException("Simulated SQL error"),
    )

    print("âœ“ Error simulation rules configured")

    # Test error scenarios
    try:
        spark.table("error_table")
    except AnalysisException as e:
        print(f"âœ“ Caught expected error: {e}")

    try:
        spark.sql("SELECT ERROR FROM table")
    except AnalysisException as e:
        print(f"âœ“ Caught expected SQL error: {e}")

    # Performance simulation demonstration
    print_section("Performance Simulation")

    perf_sim = MockPerformanceSimulator(spark)
    perf_sim.set_slowdown(2.0)  # 2x slower
    perf_sim.set_memory_limit(1024 * 1024)  # 1MB limit

    print("âœ“ Performance simulation configured")
    print("  - Slowdown factor: 2.0x")
    print("  - Memory limit: 1MB")

    # Simulate slow operation
    start_time = time.time()
    result = perf_sim.simulate_slow_operation(lambda: df.count())
    end_time = time.time()

    print(f"âœ“ Simulated slow operation completed in {end_time - start_time:.2f}s")
    print(f"âœ“ Result: {result} rows")

    # Storage operations
    print_section("Storage Operations")

    # Save DataFrame to table
    df.write.mode("overwrite").saveAsTable("employees")
    print("âœ“ Saved DataFrame to table 'employees'")

    # Query from table
    table_df = spark.table("employees")
    print(f"âœ“ Loaded from table: {table_df.count()} rows")

    # SQL operations
    sql_df = spark.sql(
        "SELECT name, is_active FROM employees WHERE is_active = true LIMIT 5"
    )
    print("âœ“ SQL query executed")
    sql_df.show()

    # Catalog operations
    print_subsection("Catalog Operations")
    databases = spark.catalog.listDatabases()
    tables = spark.catalog.listTables("default")

    print(f"âœ“ Databases: {len(databases)}")
    print(f"âœ“ Tables: {len(tables)}")
    for table in tables:
        print(f"  - {table}")

    # Testing utilities demonstration
    print_section("Testing Utilities")

    # Create a simple test DataFrame
    test_data = [{"id": 1, "name": "Test", "value": 100}]
    test_df = spark.createDataFrame(test_data)
    print(f"âœ“ Created test DataFrame: {test_df.count()} rows")

    # Create another test session
    test_session = SparkSession("TestSession")
    print(f"âœ“ Created test session: {test_session.app_name}")

    # Final statistics
    print_section("Final Statistics")

    print("âœ“ Total DataFrames created: 8+")
    print("âœ“ Total operations performed: 50+")
    print("âœ“ Data types tested: 15+")
    print("âœ“ Functions tested: 30+")
    print("âœ“ Error scenarios tested: 2")
    print("âœ“ Performance scenarios tested: 1")

    # Cleanup
    spark.stop()
    print("\nâœ… Demo completed successfully!")
    print("ðŸŽ‰ Mock Spark v2.0.0 is production-ready with 515 tests passing!")
    print("   100% Zero Raw SQL | Database Agnostic | Pure SQLAlchemy")


if __name__ == "__main__":
    main()
