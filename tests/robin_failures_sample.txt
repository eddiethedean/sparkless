

=== run_robin_failure_sample.py ===
/Users/odosmatthews/.pyenv/versions/3.11.13/bin/python -m pytest tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_union tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_subquery tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_case_when tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_like tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_in_clause tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_inner_join tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_left_join tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_order_by tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_limit tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_having tests/examples/test_unified_infrastructure_example.py::TestUnifiedInfrastructure::test_comparison tests/examples/test_unified_infrastructure_example.py::TestUnifiedInfrastructure::test_with_backend_info -v --tb=long -p no:cacheprovider

============================= test session starts ==============================
platform darwin -- Python 3.11.13, pytest-8.4.2, pluggy-1.6.0 -- /Users/odosmatthews/.pyenv/versions/3.11.13/bin/python
hypothesis profile 'ci' -> database=None, deadline=None, print_blob=True, derandomize=True, suppress_health_check=(HealthCheck.too_slow,)
rootdir: /Users/odosmatthews/Documents/coding/sparkless
configfile: pyproject.toml
plugins: anyio-4.11.0, cov-7.0.0, green-light-0.2.0, asyncio-1.2.0, xdist-3.8.0, hypothesis-6.148.7, alt-pytest-asyncio-0.9.3, async-sqlalchemy-0.2.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 12 items

tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_union <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [  8%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_subquery <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 16%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_case_when <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 25%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_like <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 33%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_in_clause <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 41%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_inner_join <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 50%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_left_join <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 58%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_order_by <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 66%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_limit <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 75%]
tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_having <- ../mock-spark/tests/parity/sql/test_advanced.py FAILED [ 83%]
tests/examples/test_unified_infrastructure_example.py::TestUnifiedInfrastructure::test_comparison <- ../mock-spark/tests/examples/test_unified_infrastructure_example.py FAILED [ 91%]
tests/examples/test_unified_infrastructure_example.py::TestUnifiedInfrastructure::test_with_backend_info <- ../mock-spark/tests/examples/test_unified_infrastructure_example.py FAILED [100%]

=================================== FAILURES ===================================
__________________ TestSQLAdvancedParity.test_sql_with_union ___________________

self = <sparkless.session.sql.executor.SQLExecutor object at 0x10cab8b50>
query = '\n            SELECT name, age FROM union_table1\n            UNION\n            SELECT name, age FROM union_table2\n        '

    def execute(self, query: str) -> IDataFrame:
        """Execute SQL query.
    
        Args:
            query: SQL query string.
    
        Returns:
            DataFrame with query results.
    
        Raises:
            QueryExecutionException: If query execution fails.
        """
        try:
            # Parse the query
            ast = self.parser.parse(query)
    
            # Execute based on query type
            if ast.query_type == "SELECT":
                return self._execute_select(ast)
            elif ast.query_type == "UNION":
>               return self._execute_union(ast)
                       ^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/session/sql/executor.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.session.sql.executor.SQLExecutor object at 0x10cab8b50>
ast = SQLAST(type='UNION', components=13)

    def _execute_union(self, ast: SQLAST) -> IDataFrame:
        """Execute UNION query.
    
        Args:
            ast: Parsed SQL AST.
    
        Returns:
            DataFrame with UNION results.
        """
        components = ast.components
    
        # Get left and right queries
        left_query = components.get("left_query", "")
        right_query = components.get("right_query", "")
    
        if not left_query or not right_query:
            raise QueryExecutionException("UNION requires two SELECT statements")
    
        # Execute both SELECT queries
        left_ast = self.parser.parse(left_query)
        right_ast = self.parser.parse(right_query)
    
        if left_ast.query_type != "SELECT" or right_ast.query_type != "SELECT":
            raise QueryExecutionException("UNION can only combine SELECT statements")
    
        left_df = self._execute_select(left_ast)
        right_df = self._execute_select(right_ast)
    
        # Convert to DataFrame if needed
        from ...dataframe import DataFrame
    
        if not isinstance(left_df, DataFrame):  # type: ignore[unreachable]
            from ...spark_types import StructType
    
            schema = (
                StructType(left_df.schema.fields)  # type: ignore[arg-type]
                if hasattr(left_df.schema, "fields")
                else StructType([])
            )
            left_df = DataFrame(left_df.collect(), schema)  # type: ignore[assignment]
    
        if not isinstance(right_df, DataFrame):  # type: ignore[unreachable]
            from ...spark_types import StructType
    
            schema = (
                StructType(right_df.schema.fields)  # type: ignore[arg-type]
                if hasattr(right_df.schema, "fields")
                else StructType([])
            )
            right_df = DataFrame(right_df.collect(), schema)  # type: ignore[assignment]
    
        # Perform union (removes duplicates like UNION, not UNION ALL)
        result = cast("DataFrame", left_df.union(right_df))
    
        # Materialize if lazy (union() may queue operations)
        if hasattr(result, "_materialize_if_lazy"):
>           result = cast("DataFrame", result._materialize_if_lazy())
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/session/sql/executor.py:1343: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[4 rows, 2 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[4 rows, 2 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
                can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )
    
                if not can_handle_all:
                    # Fail fast with clear error message
                    from ..core.exceptions.operation import (
                        SparkUnsupportedOperationError,
                    )
    
>                   raise SparkUnsupportedOperationError(
                        operation=f"Operations: {', '.join(unsupported_ops)}",
                        reason=f"Backend '{backend_type}' does not support these operations",
                        alternative="Consider using _materialize_manual() or a different backend",
                    )
E                   sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: select, union' is not supported
E                   Reason: Backend 'robin' does not support these operations
E                   Alternative: Consider using _materialize_manual() or a different backend

sparkless/dataframe/lazy.py:496: SparkUnsupportedOperationError

During handling of the above exception, another exception occurred:

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3de850>
spark = <sparkless.session.core.session.SparkSession object at 0x10ce298d0>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.session.core.session.SparkSession object at 0x10ce298d0>
query = '\n            SELECT name, age FROM union_table1\n            UNION\n            SELECT name, age FROM union_table2\n        '
args = (), kwargs = {}

    def sql(self, query: str, *args: Any, **kwargs: Any) -> IDataFrame:
        """Execute SQL query with optional parameters (mockable version).
    
        Args:
            query: SQL query string with optional placeholders.
            *args: Positional parameters for ? placeholders.
            **kwargs: Named parameters for :name placeholders.
    
        Returns:
            DataFrame with query results.
    
        Example:
            >>> spark.sql("SELECT * FROM users WHERE age > ?", 18)
            >>> spark.sql("SELECT * FROM users WHERE age > :min_age", min_age=18)
        """
>       return self._sql_impl(query, *args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/session/core/session.py:331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.session.core.session.SparkSession object at 0x10ce298d0>
query = '\n            SELECT name, age FROM union_table1\n            UNION\n            SELECT name, age FROM union_table2\n        '
args = (), kwargs = {}

    def _real_sql(self, query: str, *args: Any, **kwargs: Any) -> IDataFrame:
        """Execute SQL query with optional parameters.
    
        Args:
            query: SQL query string with optional placeholders.
            *args: Positional parameters for ? placeholders.
            **kwargs: Named parameters for :name placeholders.
    
        Returns:
            DataFrame with query results.
    
        Example:
            >>> df = spark.sql("SELECT * FROM users WHERE age > ?", 18)
            >>> df = spark.sql("SELECT * FROM users WHERE name = :name", name="Alice")
        """
        # Process parameters if provided
        if args or kwargs:
            query = self._sql_parameter_binder.bind_parameters(query, args, kwargs)
    
>       return self._sql_executor.execute(query)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/session/core/session.py:352: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.session.sql.executor.SQLExecutor object at 0x10cab8b50>
query = '\n            SELECT name, age FROM union_table1\n            UNION\n            SELECT name, age FROM union_table2\n        '

    def execute(self, query: str) -> IDataFrame:
        """Execute SQL query.
    
        Args:
            query: SQL query string.
    
        Returns:
            DataFrame with query results.
    
        Raises:
            QueryExecutionException: If query execution fails.
        """
        try:
            # Parse the query
            ast = self.parser.parse(query)
    
            # Execute based on query type
            if ast.query_type == "SELECT":
                return self._execute_select(ast)
            elif ast.query_type == "UNION":
                return self._execute_union(ast)
            elif ast.query_type == "CREATE":
                return self._execute_create(ast)
            elif ast.query_type == "DROP":
                return self._execute_drop(ast)
            elif ast.query_type == "MERGE":
                return self._execute_merge(ast)
            elif ast.query_type == "INSERT":
                return self._execute_insert(ast)
            elif ast.query_type == "UPDATE":
                return self._execute_update(ast)
            elif ast.query_type == "DELETE":
                return self._execute_delete(ast)
            elif ast.query_type == "SHOW":
                return self._execute_show(ast)
            elif ast.query_type == "DESCRIBE":
                return self._execute_describe(ast)
            elif ast.query_type == "REFRESH":
                return self._execute_refresh(ast)
            else:
                raise QueryExecutionException(
                    f"Unsupported query type: {ast.query_type}"
                )
    
        except Exception as e:
            if isinstance(e, QueryExecutionException):
                raise
>           raise QueryExecutionException(f"Failed to execute query: {str(e)}")
E           sparkless.core.exceptions.execution.QueryExecutionException: Failed to execute query: Operation 'Operations: select, union' is not supported
E           Reason: Backend 'robin' does not support these operations
E           Alternative: Consider using _materialize_manual() or a different backend

sparkless/session/sql/executor.py:150: QueryExecutionException
_________________ TestSQLAdvancedParity.test_sql_with_subquery _________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3def90>
spark = <sparkless.session.core.session.SparkSession object at 0x10e4f5d10>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:164: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def collect(self) -> List["Row"]:
        """Collect all data as list of Row objects."""
        # Materialize if needed (this updates the schema and data)
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[6 rows, 2 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
>               can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )

sparkless/dataframe/lazy.py:486: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.backend.robin.materializer.RobinMaterializer object at 0x10e4f6f90>
operations = [('filter', <sparkless.functions.core.column.ColumnOperation object at 0x10ca7ee90>), ('select', (<sparkless.functions.core.column.Column object at 0x10caf2510>, <sparkless.functions.core.column.Column object at 0x10cad6950>))]

    def can_handle_operations(
        self, operations: List[Tuple[str, Any]]
    ) -> Tuple[bool, List[str]]:
        unsupported: List[str] = []
        for op_name, op_payload in operations:
>           if not self.can_handle_operation(op_name, op_payload):
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/backend/robin/materializer.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.backend.robin.materializer.RobinMaterializer object at 0x10e4f6f90>
op_name = 'filter'
op_payload = <sparkless.functions.core.column.ColumnOperation object at 0x10ca7ee90>

    def can_handle_operation(self, op_name: str, op_payload: Any) -> bool:
        if op_name not in self.SUPPORTED_OPERATIONS:
            return False
        if op_name == "filter":
>           return self._can_handle_filter(op_payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/backend/robin/materializer.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.backend.robin.materializer.RobinMaterializer object at 0x10e4f6f90>
payload = <sparkless.functions.core.column.ColumnOperation object at 0x10ca7ee90>

    def _can_handle_filter(self, payload: Any) -> bool:
>       return _simple_filter_to_robin(payload) is not None
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/backend/robin/materializer.py:92: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

condition = <sparkless.functions.core.column.ColumnOperation object at 0x10ca7ee90>

    def _simple_filter_to_robin(condition: Any) -> Any:
        """Translate a simple Sparkless filter to robin_sparkless Column expression.
    
        Only supports: ColumnOperation with op in (>, <, >=, <=, ==, !=),
        column = Column(name), value = scalar or Literal.
        Returns None if not supported.
        """
        from sparkless.functions import ColumnOperation
        from sparkless.functions.core.column import Column
        from sparkless.functions.core.literals import Literal
    
        if not _robin_available():
            return None
        F = robin_sparkless  # type: ignore[union-attr]
    
        if isinstance(condition, ColumnOperation):
            op = getattr(condition, "operation", None)
            col_side = getattr(condition, "column", None)
            val_side = getattr(condition, "value", None)
            if op not in (">", "<", ">=", "<=", "==", "!="):
                return None
            # Left side: Column (col name) or ColumnOperation (nested, not supported here)
            if not isinstance(col_side, Column):
                return None
            col_name = getattr(col_side, "name", None)
            if not isinstance(col_name, str):
                return None
            robin_col = F.col(col_name)
            # Right side: Literal or scalar
            if isinstance(val_side, Literal):
                val = getattr(val_side, "value", val_side)
            else:
                val = val_side
            robin_lit = F.lit(val)
            if op == ">":
>               return robin_col > robin_lit
                       ^^^^^^^^^^^^^^^^^^^^^
E               TypeError: '>' not supported between instances of 'builtins.Column' and 'builtins.Column'

sparkless/backend/robin/materializer.py:65: TypeError
________________ TestSQLAdvancedParity.test_sql_with_case_when _________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3df6d0>
spark = <sparkless.session.core.session.SparkSession object at 0x10d44c310>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 3 columns]

    def collect(self) -> List["Row"]:
        """Collect all data as list of Row objects."""
        # Materialize if needed (this updates the schema and data)
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 3 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[6 rows, 3 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
                can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )
    
                if not can_handle_all:
                    # Fail fast with clear error message
                    from ..core.exceptions.operation import (
                        SparkUnsupportedOperationError,
                    )
    
>                   raise SparkUnsupportedOperationError(
                        operation=f"Operations: {', '.join(unsupported_ops)}",
                        reason=f"Backend '{backend_type}' does not support these operations",
                        alternative="Consider using _materialize_manual() or a different backend",
                    )
E                   sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: select' is not supported
E                   Reason: Backend 'robin' does not support these operations
E                   Alternative: Consider using _materialize_manual() or a different backend

sparkless/dataframe/lazy.py:496: SparkUnsupportedOperationError
___________________ TestSQLAdvancedParity.test_sql_with_like ___________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3dfe10>
spark = <sparkless.session.core.session.SparkSession object at 0x10cad4550>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[8 rows, 1 columns]

    def collect(self) -> List["Row"]:
        """Collect all data as list of Row objects."""
        # Materialize if needed (this updates the schema and data)
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[8 rows, 1 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[8 rows, 1 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
                can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )
    
                if not can_handle_all:
                    # Fail fast with clear error message
                    from ..core.exceptions.operation import (
                        SparkUnsupportedOperationError,
                    )
    
>                   raise SparkUnsupportedOperationError(
                        operation=f"Operations: {', '.join(unsupported_ops)}",
                        reason=f"Backend '{backend_type}' does not support these operations",
                        alternative="Consider using _materialize_manual() or a different backend",
                    )
E                   sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: filter' is not supported
E                   Reason: Backend 'robin' does not support these operations
E                   Alternative: Consider using _materialize_manual() or a different backend

sparkless/dataframe/lazy.py:496: SparkUnsupportedOperationError
________________ TestSQLAdvancedParity.test_sql_with_in_clause _________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3e4550>
spark = <sparkless.session.core.session.SparkSession object at 0x10caf0e90>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:222: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def collect(self) -> List["Row"]:
        """Collect all data as list of Row objects."""
        # Materialize if needed (this updates the schema and data)
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[6 rows, 2 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
                can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )
    
                if not can_handle_all:
                    # Fail fast with clear error message
                    from ..core.exceptions.operation import (
                        SparkUnsupportedOperationError,
                    )
    
>                   raise SparkUnsupportedOperationError(
                        operation=f"Operations: {', '.join(unsupported_ops)}",
                        reason=f"Backend '{backend_type}' does not support these operations",
                        alternative="Consider using _materialize_manual() or a different backend",
                    )
E                   sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: filter' is not supported
E                   Reason: Backend 'robin' does not support these operations
E                   Alternative: Consider using _materialize_manual() or a different backend

sparkless/dataframe/lazy.py:496: SparkUnsupportedOperationError
________________ TestSQLAdvancedParity.test_sql_with_inner_join ________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3cbe50>
spark = <sparkless.session.core.session.SparkSession object at 0x10caf3090>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def collect(self) -> List["Row"]:
        """Collect all data as list of Row objects."""
        # Materialize if needed (this updates the schema and data)
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[6 rows, 2 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
                can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )
    
                if not can_handle_all:
                    # Fail fast with clear error message
                    from ..core.exceptions.operation import (
                        SparkUnsupportedOperationError,
                    )
    
>                   raise SparkUnsupportedOperationError(
                        operation=f"Operations: {', '.join(unsupported_ops)}",
                        reason=f"Backend '{backend_type}' does not support these operations",
                        alternative="Consider using _materialize_manual() or a different backend",
                    )
E                   sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: join, select' is not supported
E                   Reason: Backend 'robin' does not support these operations
E                   Alternative: Consider using _materialize_manual() or a different backend

sparkless/dataframe/lazy.py:496: SparkUnsupportedOperationError
________________ TestSQLAdvancedParity.test_sql_with_left_join _________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3dcb10>
spark = <sparkless.session.core.session.SparkSession object at 0x10cae8a90>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def collect(self) -> List["Row"]:
        """Collect all data as list of Row objects."""
        # Materialize if needed (this updates the schema and data)
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[6 rows, 2 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
                can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )
    
                if not can_handle_all:
                    # Fail fast with clear error message
                    from ..core.exceptions.operation import (
                        SparkUnsupportedOperationError,
                    )
    
>                   raise SparkUnsupportedOperationError(
                        operation=f"Operations: {', '.join(unsupported_ops)}",
                        reason=f"Backend '{backend_type}' does not support these operations",
                        alternative="Consider using _materialize_manual() or a different backend",
                    )
E                   sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: join, select' is not supported
E                   Reason: Backend 'robin' does not support these operations
E                   Alternative: Consider using _materialize_manual() or a different backend

sparkless/dataframe/lazy.py:496: SparkUnsupportedOperationError
_________________ TestSQLAdvancedParity.test_sql_with_order_by _________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3dd250>
spark = <sparkless.session.core.session.SparkSession object at 0x10e3dfed0>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def collect(self) -> List["Row"]:
        """Collect all data as list of Row objects."""
        # Materialize if needed (this updates the schema and data)
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[6 rows, 2 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[6 rows, 2 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
                can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )
    
                if not can_handle_all:
                    # Fail fast with clear error message
                    from ..core.exceptions.operation import (
                        SparkUnsupportedOperationError,
                    )
    
>                   raise SparkUnsupportedOperationError(
                        operation=f"Operations: {', '.join(unsupported_ops)}",
                        reason=f"Backend '{backend_type}' does not support these operations",
                        alternative="Consider using _materialize_manual() or a different backend",
                    )
E                   sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: orderBy' is not supported
E                   Reason: Backend 'robin' does not support these operations
E                   Alternative: Consider using _materialize_manual() or a different backend

sparkless/dataframe/lazy.py:496: SparkUnsupportedOperationError
__________________ TestSQLAdvancedParity.test_sql_with_limit ___________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3dd9d0>
spark = <sparkless.session.core.session.SparkSession object at 0x10e492310>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[8 rows, 2 columns]

    def count(self) -> int:
        """Count number of rows."""
        # Materialize if needed
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:575: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[8 rows, 2 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[8 rows, 2 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
                can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )
    
                if not can_handle_all:
                    # Fail fast with clear error message
                    from ..core.exceptions.operation import (
                        SparkUnsupportedOperationError,
                    )
    
                    raise SparkUnsupportedOperationError(
                        operation=f"Operations: {', '.join(unsupported_ops)}",
                        reason=f"Backend '{backend_type}' does not support these operations",
                        alternative="Consider using _materialize_manual() or a different backend",
                    )
    
                # Compute final schema after all operations
                from ..dataframe.schema.schema_manager import SchemaManager
    
                final_schema = SchemaManager.project_schema_with_operations(
                    df.schema, df._operations_queue
                )
    
                # Optional plan-based path: use materialize_from_plan when backend supports it
                # and session is configured (spark.sparkless.useLogicalPlan=true) or backend is robin
                use_plan = getattr(materializer, "materialize_from_plan", None)
                if use_plan is not None:
                    try:
                        from sparkless.session.core.session import SparkSession as SS
    
                        active = getattr(SS, "_active_sessions", [])
                        session = active[-1] if active else None
                        conf_val = (
                            session.conf.get("spark.sparkless.useLogicalPlan", "false")
                            if session and hasattr(session, "conf")
                            else "false"
                        )
                        use_plan_flag = str(conf_val).lower() in ("true", "1", "yes")
                    except (AttributeError, TypeError):
                        use_plan_flag = False
                    if use_plan_flag or backend_type == "robin":
                        from ..dataframe.logical_plan import to_logical_plan
    
                        try:
                            logical_plan = to_logical_plan(df)
                            rows = use_plan(df.data, df.schema, logical_plan)
                        except (ValueError, TypeError):
                            # Plan path doesn't support this plan (e.g. window, opaque, CaseWhen)
                            rows = materializer.materialize(
                                df.data, df.schema, df._operations_queue
                            )
                    else:
                        rows = materializer.materialize(
                            df.data, df.schema, df._operations_queue
                        )
                else:
>                   rows = materializer.materialize(
                        df.data, df.schema, df._operations_queue
                    )

sparkless/dataframe/lazy.py:542: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.backend.robin.materializer.RobinMaterializer object at 0x10e3df210>
data = [{'age': 25, 'name': 'Alice'}, {'age': 30, 'name': 'Bob'}, {'age': 35, 'name': 'Charlie'}, {'age': 40, 'name': 'David'}, {'age': 25, 'name': 'Alice'}, {'age': 30, 'name': 'Bob'}, ...]
schema = StructType([StructField(name='name', dataType=StringType(nullable=True), nullable=True), StructField(name='age', dataType=LongType(nullable=True), nullable=True)])
operations = [('limit', 2)]

    def materialize(
        self,
        data: List[Any],
        schema: StructType,
        operations: List[Tuple[str, Any]],
    ) -> List[Row]:
        if not _robin_available():
            raise RuntimeError(
                "robin_sparkless is not installed. "
                "Install with: pip install sparkless[robin] (or pip install robin-sparkless)."
            )
        F = robin_sparkless  # type: ignore[union-attr]
        # PoC: only 3-column schema
        if not schema.fields or len(schema.fields) != 3:
>           raise ValueError(
                "RobinMaterializer PoC only supports schema with exactly 3 columns"
            )
E           ValueError: RobinMaterializer PoC only supports schema with exactly 3 columns

sparkless/backend/robin/materializer.py:130: ValueError
__________________ TestSQLAdvancedParity.test_sql_with_having __________________

self = <tests.parity.sql.test_advanced.TestSQLAdvancedParity object at 0x10e3de110>
spark = <sparkless.session.core.session.SparkSession object at 0x10e4926d0>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/parity/sql/test_advanced.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[2 rows, 2 columns]

    def collect(self) -> List["Row"]:
        """Collect all data as list of Row objects."""
        # Materialize if needed (this updates the schema and data)
        if self._operations_queue:
>           materialized = self._materialize_if_lazy()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DataFrame[2 rows, 2 columns]

    def _materialize_if_lazy(self) -> SupportsDataFrameOps:
        """Materialize lazy operations if any are queued."""
        if self._operations_queue:
            lazy_engine = self._get_lazy_engine()
>           result = cast("SupportsDataFrameOps", lazy_engine.materialize(self))
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/dataframe/dataframe.py:926: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df = DataFrame[2 rows, 2 columns]

    @staticmethod
    def materialize(df: "DataFrame") -> "DataFrame":
        """Materialize queued lazy operations.
    
        Args:
            df: Lazy DataFrame with queued operations
    
        Returns:
            Eager DataFrame with operations applied
        """
        if not df._operations_queue:
            from ..dataframe import DataFrame
    
            return DataFrame(df.data, df.schema, df.storage)
    
        # Check if operations require manual materialization
        if LazyEvaluationEngine._requires_manual_materialization(df._operations_queue):
            return LazyEvaluationEngine._materialize_manual(df)
        # Use backend factory to get materializer
        try:
            from sparkless.backend.factory import BackendFactory
    
            # Detect backend type from DataFrame's storage
            backend_type = BackendFactory.get_backend_type(df.storage)
            materializer = BackendFactory.create_materializer(backend_type)
            try:
                # Check capabilities upfront before materialization
>               can_handle_all, unsupported_ops = materializer.can_handle_operations(
                    df._operations_queue
                )

sparkless/dataframe/lazy.py:486: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.backend.robin.materializer.RobinMaterializer object at 0x110110310>
operations = [('filter', <sparkless.functions.core.column.ColumnOperation object at 0x110110a90>), ('select', ('dept', 'avg_salary'))]

    def can_handle_operations(
        self, operations: List[Tuple[str, Any]]
    ) -> Tuple[bool, List[str]]:
        unsupported: List[str] = []
        for op_name, op_payload in operations:
>           if not self.can_handle_operation(op_name, op_payload):
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/backend/robin/materializer.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.backend.robin.materializer.RobinMaterializer object at 0x110110310>
op_name = 'filter'
op_payload = <sparkless.functions.core.column.ColumnOperation object at 0x110110a90>

    def can_handle_operation(self, op_name: str, op_payload: Any) -> bool:
        if op_name not in self.SUPPORTED_OPERATIONS:
            return False
        if op_name == "filter":
>           return self._can_handle_filter(op_payload)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/backend/robin/materializer.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sparkless.backend.robin.materializer.RobinMaterializer object at 0x110110310>
payload = <sparkless.functions.core.column.ColumnOperation object at 0x110110a90>

    def _can_handle_filter(self, payload: Any) -> bool:
>       return _simple_filter_to_robin(payload) is not None
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

sparkless/backend/robin/materializer.py:92: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

condition = <sparkless.functions.core.column.ColumnOperation object at 0x110110a90>

    def _simple_filter_to_robin(condition: Any) -> Any:
        """Translate a simple Sparkless filter to robin_sparkless Column expression.
    
        Only supports: ColumnOperation with op in (>, <, >=, <=, ==, !=),
        column = Column(name), value = scalar or Literal.
        Returns None if not supported.
        """
        from sparkless.functions import ColumnOperation
        from sparkless.functions.core.column import Column
        from sparkless.functions.core.literals import Literal
    
        if not _robin_available():
            return None
        F = robin_sparkless  # type: ignore[union-attr]
    
        if isinstance(condition, ColumnOperation):
            op = getattr(condition, "operation", None)
            col_side = getattr(condition, "column", None)
            val_side = getattr(condition, "value", None)
            if op not in (">", "<", ">=", "<=", "==", "!="):
                return None
            # Left side: Column (col name) or ColumnOperation (nested, not supported here)
            if not isinstance(col_side, Column):
                return None
            col_name = getattr(col_side, "name", None)
            if not isinstance(col_name, str):
                return None
            robin_col = F.col(col_name)
            # Right side: Literal or scalar
            if isinstance(val_side, Literal):
                val = getattr(val_side, "value", val_side)
            else:
                val = val_side
            robin_lit = F.lit(val)
            if op == ">":
>               return robin_col > robin_lit
                       ^^^^^^^^^^^^^^^^^^^^^
E               TypeError: '>' not supported between instances of 'builtins.Column' and 'builtins.Column'

sparkless/backend/robin/materializer.py:65: TypeError
__________________ TestUnifiedInfrastructure.test_comparison ___________________

self = <test_unified_infrastructure_example.TestUnifiedInfrastructure object at 0x10e3e6f90>
mock_spark_session = <sparkless.session.core.session.SparkSession object at 0x10cad7a50>
pyspark_session = <pyspark.sql.session.SparkSession object at 0x10ffded90>

>   ???

/Users/odosmatthews/Documents/coding/mock-spark/tests/examples/test_unified_infrastructure_example.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

mock_df = DataFrame[2 rows, 2 columns]
pyspark_df = DataFrame[id: bigint, value: bigint], tolerance = 1e-06
check_schema = True, check_order = False, msg = None

    def assert_dataframes_equal(
        mock_df: Any,
        pyspark_df: Any,
        tolerance: float = 1e-6,
        check_schema: bool = True,
        check_order: bool = False,
        msg: Optional[str] = None,
    ) -> None:
        """Assert that two DataFrames are equal.
    
        Args:
            mock_df: mock-spark DataFrame.
            pyspark_df: PySpark DataFrame.
            tolerance: Tolerance for floating point comparisons.
            check_schema: Whether to compare schemas.
            check_order: Whether row order must match.
            msg: Optional custom error message.
    
        Raises:
            AssertionError: If DataFrames are not equal.
        """
        is_equal, error_msg = compare_dataframes(
            mock_df, pyspark_df, tolerance, check_schema, check_order
        )
    
        if not is_equal:
            error = error_msg or "DataFrames are not equal"
            if msg:
                error = f"{msg}: {error}"
>           raise AssertionError(error)
E           AssertionError: Failed to count rows: '>' not supported between instances of 'builtins.Column' and 'builtins.Column'

tests/fixtures/comparison.py:319: AssertionError
---------------------------- Captured stderr setup -----------------------------
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/05 17:14:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
_______________ TestUnifiedInfrastructure.test_with_backend_info _______________

self = <test_unified_infrastructure_example.TestUnifiedInfrastructure object at 0x10e3e7bd0>
spark = <sparkless.session.core.session.SparkSession object at 0x10cad4310>
spark_backend = <BackendType.ROBIN: 'robin'>

>   ???
E   AssertionError: assert 'robin' in ['mock', 'pyspark']

/Users/odosmatthews/Documents/coding/mock-spark/tests/examples/test_unified_infrastructure_example.py:97: AssertionError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.11.13-final-0 _______________

Name                                                                 Stmts   Miss  Cover   Missing
--------------------------------------------------------------------------------------------------
sparkless/__init__.py                                                   24      0   100%
sparkless/_version.py                                                    8      4    50%   10-12, 16-19
sparkless/backend/__init__.py                                            3      0   100%
sparkless/backend/factory.py                                           131     71    46%   51-53, 55-63, 72-74, 76-79, 82, 90, 114-116, 118-126, 129-131, 134-136, 139, 147, 162-196, 216-238, 249, 266, 280-281, 296-297
sparkless/backend/polars/__init__.py                                     4      0   100%
sparkless/backend/polars/_over_compat.py                                19     12    37%   21-27, 36-40
sparkless/backend/polars/executors/__init__.py                           1      1     0%   13
sparkless/backend/polars/export.py                                      36     28    22%   27-53, 67-76, 88-89, 102-103, 113-114
sparkless/backend/polars/expression_translator.py                     2118   2069     2%   40-48, 59-148, 161-167, 178-192, 201-214, 236-308, 327-362, 378-383, 395-399, 417-1011, 1033-1170, 1201-1277, 1289-1499, 1514, 1519-1523, 1526-1565, 1568-1581, 1584-1588, 1591-1595, 1603-1604, 1625-4438, 4450-4467, 4480-4490, 4503-4529, 4542-4596, 4607-4653, 4666-4694, 4705-4729, 4742-4897, 4908-4957, 4969-4976, 4988-5022, 5035-5068, 5083-5093
sparkless/backend/polars/materializer.py                               813    790     3%   48-49, 58-68, 75-105, 125-1723, 1735-1750, 1766-1781, 1793-1800, 1815-1850, 1865-1871, 1880-1882, 1887
sparkless/backend/polars/operation_executor.py                        2032   1981     3%   37-42, 56-113, 121-130, 139-149, 164-170, 176-177, 191-347, 360-1983, 1992-2000, 2005-2020, 2025-2046, 2051-2055, 2060-2068, 2071-2084, 2087-2102, 2105-2108, 2113-2125, 2128-2147, 2169-3372, 3388-3434, 3466-3610, 3633-3904, 3934-4223, 4241-4313, 4326-4347, 4362-4420, 4432, 4444, 4461-4484, 4495, 4507, 4522
sparkless/backend/polars/parquet_storage.py                             41     41     0%   8-119
sparkless/backend/polars/plan_interpreter.py                           381    381     0%   8-531
sparkless/backend/polars/schema_registry.py                             73     57    22%   42-43, 59-94, 110-190, 203-207, 217-222
sparkless/backend/polars/schema_utils.py                                31     12    61%   18-20, 29-38, 53, 65
sparkless/backend/polars/storage.py                                    350    190    46%   61, 68-69, 80-83, 87-94, 98-103, 108, 118, 127, 149-152, 164, 170-171, 183-185, 189-196, 200-206, 225-226, 242, 274, 285-292, 301-319, 325-349, 367, 375-377, 385-400, 420, 440-457, 477-481, 494-538, 553, 561, 574, 578, 607, 611, 626-634, 669, 681-690, 702-703, 707, 714, 718, 724, 728, 730, 735, 740-743, 753-771
sparkless/backend/polars/translators/__init__.py                         1      1     0%   14
sparkless/backend/polars/translators/arithmetic_translator.py           17     17     0%   8-70
sparkless/backend/polars/translators/string_translator.py              145    145     0%   8-302
sparkless/backend/polars/translators/type_translator.py                107    107     0%   8-190
sparkless/backend/polars/type_mapper.py                                 97     88     9%   46, 49-92, 107-169
sparkless/backend/polars/window_handler.py                             290    278     4%   20-25, 30-44, 56-70, 80-85, 100-113, 132-563
sparkless/backend/protocols.py                                          14      0   100%
sparkless/backend/robin/__init__.py                                      4      0   100%
sparkless/backend/robin/export.py                                       15      7    53%   18-20, 23, 26, 31, 40, 43
sparkless/backend/robin/materializer.py                                112     55    51%   22-23, 42, 53, 56, 60, 66-76, 102, 105, 123, 133-175
sparkless/backend/robin/storage.py                                      43     11    74%   26, 29, 35, 46, 52, 67, 72, 78, 84, 89, 94
sparkless/compat/__init__.py                                             2      0   100%
sparkless/compat/datetime.py                                            74     53    28%   43, 55-57, 61-67, 73-81, 96-103, 115-122, 128-133, 141-146, 162-182
sparkless/config.py                                                     55     17    69%   45-57, 63-64, 68, 76-80, 89-90, 105
sparkless/core/__init__.py                                               9      0   100%
sparkless/core/column_resolver.py                                       33     11    67%   48, 72-73, 105-116, 139
sparkless/core/condition_evaluator.py                                  764    730     4%   27-34, 49-56, 71-245, 260-541, 556-576, 591-604, 619-781, 795-1203, 1218-1229, 1245-1274, 1290-1311, 1324-1340, 1353, 1366-1370, 1384-1403
sparkless/core/data_validation.py                                       80     47    41%   56-85, 91-113, 129, 136-137, 161, 168, 173-187, 205-206, 220-221
sparkless/core/ddl_adapter.py                                           31     23    26%   52-53, 65-66, 78-79, 91-106, 118-133
sparkless/core/exceptions/__init__.py                                    6      0   100%
sparkless/core/exceptions/analysis.py                                  100     81    19%   37-63, 70-73, 91, 109, 137-162, 170-202, 230-245, 269-272, 302-323
sparkless/core/exceptions/base.py                                       22      4    82%   31, 61, 76, 91
sparkless/core/exceptions/execution.py                                  29     12    59%   45, 63, 87-90, 114-117, 135, 153
sparkless/core/exceptions/operation.py                                  87     55    37%   24-34, 49-58, 72-82, 96-106, 120-129, 152, 173-183
sparkless/core/exceptions/py4j_compat.py                                10     10     0%   8-41
sparkless/core/exceptions/runtime.py                                    41     25    39%   27, 53-57, 75, 101-107, 131-134, 158-161, 187-193
sparkless/core/exceptions/validation.py                                 44     26    41%   27, 45, 63, 81, 107-111, 141-147, 171-174, 202-207
sparkless/core/interfaces/__init__.py                                    5      0   100%
sparkless/core/interfaces/dataframe.py                                 180     56    69%   20, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81, 86, 91, 98, 103, 110, 115, 120, 125, 130, 136, 142, 151, 156, 161, 166, 171, 176, 181, 190, 195, 200, 205, 212, 217, 222, 227, 232, 241, 246, 251, 256, 261, 266, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320
sparkless/core/interfaces/functions.py                                 186     56    70%   23, 28, 33, 42, 47, 56, 61, 70, 75, 84, 89, 94, 99, 108, 113, 118, 123, 130, 135, 144, 149, 154, 159, 164, 173, 178, 185, 192, 197, 202, 207, 212, 221, 226, 231, 236, 241, 250, 255, 263, 273, 278, 283, 288, 293, 298, 303, 308, 313, 318, 323, 328, 333, 338, 343, 348
sparkless/core/interfaces/session.py                                   117     34    71%   20, 26, 32, 38, 44, 50, 59, 64, 69, 76, 81, 86, 96, 101, 106, 115, 120, 125, 136, 141, 146, 151, 156, 161, 166, 171, 176, 185, 190, 195, 200, 209, 214, 219
sparkless/core/interfaces/storage.py                                   126     38    70%   29, 34, 39, 44, 54, 59, 64, 69, 76, 83, 90, 97, 102, 106, 110, 114, 120, 126, 136, 142, 148, 153, 158, 163, 168, 173, 186, 192, 198, 204, 210, 215, 220, 229, 234, 239, 248, 253
sparkless/core/protocols.py                                             42      0   100%
sparkless/core/safe_evaluator.py                                       125    116     7%   39-50, 63-67, 83-215
sparkless/core/schema_inference.py                                     107     47    56%   73, 83-84, 99, 111, 132, 158, 161, 165, 167, 170-175, 179, 187-200, 218-237, 242-251, 256-265, 279-280, 297, 307
sparkless/core/type_utils.py                                            86     60    30%   20-21, 91-93, 112-121, 140-144, 164-168, 187-199, 223-259, 278, 290-292, 304
sparkless/core/types/__init__.py                                         4      0   100%
sparkless/core/types/data_types.py                                     139     38    73%   22, 27, 32, 37, 42, 47, 52, 57, 66, 76, 81, 91, 97, 102, 111, 121, 127, 132, 141, 146, 155, 160, 170, 175, 180, 190, 196, 201, 206, 211, 221, 226, 231, 240, 249, 254, 259, 264
sparkless/core/types/metadata.py                                       166     47    72%   18, 23, 28, 33, 38, 43, 48, 53, 58, 63, 73, 79, 85, 91, 97, 103, 108, 113, 118, 128, 134, 140, 145, 150, 160, 166, 172, 177, 182, 187, 197, 203, 209, 215, 220, 225, 230, 239, 244, 249, 254, 259, 268, 273, 278, 283, 288
sparkless/core/types/schema.py                                         121     36    70%   22, 28, 34, 40, 45, 50, 55, 60, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 185, 190, 195, 200, 209, 214, 221, 226
sparkless/data_generation/__init__.py                                    4      4     0%   8-12
sparkless/data_generation/builder.py                                    28     28     0%   8-85
sparkless/data_generation/convenience.py                                 9      9     0%   8-64
sparkless/data_generation/generator.py                                 174    174     0%   8-337
sparkless/dataframe/__init__.py                                          6      0   100%
sparkless/dataframe/aggregations/__init__.py                             2      2     0%   8-10
sparkless/dataframe/aggregations/operations.py                          75     75     0%   8-208
sparkless/dataframe/assertions/__init__.py                               3      3     0%   7-10
sparkless/dataframe/assertions/assertions.py                            26     26     0%   8-88
sparkless/dataframe/assertions/operations.py                            16     16     0%   8-45
sparkless/dataframe/attribute_handler.py                                62     30    52%   30, 54, 77-85, 116-125, 176, 190-196, 213-217, 223-224
sparkless/dataframe/casting/__init__.py                                  2      2     0%   3-5
sparkless/dataframe/casting/type_converter.py                           95     95     0%   3-180
sparkless/dataframe/collection_handler.py                               29     17    41%   27-29, 35-41, 47-53, 68-72, 81
sparkless/dataframe/condition_handler.py                                52     52     0%   8-180
sparkless/dataframe/dataframe.py                                       513    262    49%   187-191, 211-215, 219-224, 240-243, 262-265, 280, 305, 311, 317, 325, 329, 333, 339, 345, 365, 373, 377, 381, 390, 394, 408, 420, 424, 428, 432, 436, 440, 449, 453, 457, 463, 469-477, 486, 490, 498-500, 507-508, 518-532, 551, 555-561, 565, 569, 577-581, 596-609, 621, 625, 630, 634, 638, 642, 652, 674, 689-691, 700, 709, 715, 719, 723, 727, 733, 742, 746, 750, 756, 760, 770, 780, 791, 795, 799, 803, 809, 813, 817, 821, 829, 835, 839, 843, 847, 852, 856, 860, 864, 875, 894, 899, 928, 960-969, 987-1062, 1078-1097, 1103, 1128, 1139-1174, 1195, 1201-1206, 1210, 1216, 1225, 1241-1244, 1250, 1258, 1269, 1284, 1292, 1303, 1311, 1323, 1329, 1335, 1342, 1347-1353, 1368-1379, 1383, 1403-1411, 1420-1460
sparkless/dataframe/display/__init__.py                                  3      3     0%   3-6
sparkless/dataframe/display/formatter.py                                37     37     0%   3-57
sparkless/dataframe/display/operations.py                              118    118     0%   8-274
sparkless/dataframe/evaluation/__init__.py                               2      0   100%
sparkless/dataframe/evaluation/evaluators/__init__.py                    1      1     0%   13
sparkless/dataframe/evaluation/evaluators/conditional_evaluator.py      22     22     0%   7-63
sparkless/dataframe/evaluation/expression_evaluator.py                2600   2365     9%   70-78, 89-113, 117-122, 128-130, 134, 138-181, 193-259, 269-300, 306-416, 422-448, 458-1001, 1007-1046, 1052-1237, 1243-1320, 1327-1365, 1371-1422, 1428-1514, 1519-1535, 1542-1554, 1558-1573, 1577-1592, 1596-1611, 1615-1630, 1641-1646, 1650-1673, 1677, 1681, 1690-1711, 1715, 1924, 1928, 1932-1936, 1940-1944, 1948-1952, 1956-1969, 1973-1980, 1984-1994, 1998-2008, 2012-2014, 2020-2023, 2027-2030, 2034-2041, 2045-2050, 2065-2123, 2127-2132, 2143-2187, 2191-2202, 2206-2214, 2220-2222, 2226-2231, 2235-2237, 2241-2243, 2249, 2253-2259, 2263, 2267-2272, 2276-2278, 2282-2284, 2288, 2292, 2296, 2304, 2308, 2312-2315, 2319-2321, 2325-2330, 2334-2337, 2341-2353, 2360, 2364-2396, 2400-2405, 2409-2431, 2435-2457, 2463-2479, 2483-2514, 2520-2555, 2561-2568, 2572-2592, 2596-2639, 2643-2660, 2664-2677, 2682, 2686-2687, 2691, 2695, 2699, 2705-2714, 2718-2727, 2731-2738, 2744, 2748-2762, 2766-2773, 2777-2791, 2795-2802, 2806-2813, 2817-2824, 2828-2837, 2841-2850, 2854-2863, 2867-2874, 2878-2885, 2889-2896, 2900-2907, 2911-2918, 2924, 2928-2937, 2942, 2947, 2952-2984, 2988-2997, 3003, 3009, 3013-3024, 3030, 3037, 3042, 3048, 3053-3077, 3082, 3087, 3092, 3101-3133, 3137-3181, 3185-3229, 3233-3260, 3265-3286, 3294-3356, 3360-3367, 3371-3391, 3397, 3403, 3407-3522, 3527-3538, 3542-3551, 3555, 3559, 3563, 3567, 3571, 3575, 3579, 3583-3588, 3592-3598, 3602-3607, 3611-3616, 3625, 3639-3673, 3679, 3683-3690, 3694-3705, 3710-3719, 3725-3734, 3740-3745, 3751-3756, 3762-3770, 3776-3784, 3788, 3792-3798, 3802-3813, 3817-3820, 3824-3830, 3834-3836, 3841, 3846-3858, 3862, 3866-3872, 3876-3882, 3888-3898, 3902-3909, 3914, 3918-3920, 3924-3947, 3951-3974, 3979, 3984, 3989, 3994, 3999, 4004, 4008-4020, 4026-4033, 4039-4046, 4050-4059, 4065-4072, 4078-4085, 4091-4098, 4106-4120, 4124-4131, 4135-4142, 4147-4162, 4166-4181, 4201, 4206, 4211, 4217-4228, 4235-4238, 4244-4247, 4253-4258, 4263-4267, 4271-4288, 4294-4309, 4315-4330, 4336-4339, 4343-4345, 4350-4358, 4364-4391, 4396-4405, 4409-4411, 4415-4417, 4422-4441, 4445-4462, 4466-4483, 4487-4504, 4508-4513, 4519-4530, 4534-4545, 4549-4554
sparkless/dataframe/export.py                                           15     15     0%   8-46
sparkless/dataframe/grouped/__init__.py                                  5      0   100%
sparkless/dataframe/grouped/base.py                                   1053    923    12%   89, 93, 95, 101-145, 152, 176-185, 188, 192-196, 207-328, 347, 352, 357-389, 406-422, 439-442, 465-466, 515-516, 518-519, 528-529, 537-696, 710-765, 790-863, 867-891, 919-922, 929, 931-934, 938-1648, 1665-1935, 1946-1953, 1964-1971, 1986, 1997-2008, 2019-2026, 2037-2044, 2055-2064, 2075-2084, 2095-2104, 2115-2124, 2135-2144, 2155-2164, 2175-2202, 2213-2240, 2254-2282, 2304-2371, 2393-2448
sparkless/dataframe/grouped/cube.py                                     86     77    10%   30-31, 47-215
sparkless/dataframe/grouped/pivot.py                                   482    459     5%   36-39, 55-356, 362-397, 403-633, 641-802, 814-823, 834-843, 854, 865-874, 885-894, 905-914, 925-931, 942-948, 959-965, 976-982, 993-999, 1010-1016, 1027-1033
sparkless/dataframe/grouped/rollup.py                                   87     79     9%   29-30, 48-220
sparkless/dataframe/joins/__init__.py                                    2      2     0%   8-10
sparkless/dataframe/joins/operations.py                                146    146     0%   8-396
sparkless/dataframe/lazy.py                                           1140   1047     8%   59-61, 73-106, 120-126, 141-211, 224-254, 266-290, 305-341, 355-368, 386-400, 406-442, 448-457, 470-472, 476, 513-538, 547-607, 613, 654-708, 718-801, 835, 839, 845, 859-870, 892, 895, 900, 904, 907, 923-1024, 1036-2116, 2129-2390, 2403-2459, 2474-2702, 2718-2730
sparkless/dataframe/logical_plan.py                                    221    221     0%   9-452
sparkless/dataframe/operations/__init__.py                               2      0   100%
sparkless/dataframe/operations/aggregation_operations.py               128    128     0%   3-313
sparkless/dataframe/operations/join_operations.py                      157    157     0%   3-329
sparkless/dataframe/operations/misc.py                                 516    472     9%   56-77, 103-145, 167-215, 236-250, 267-308, 323-407, 421-501, 518-549, 566-593, 615-630, 643-659, 678-685, 710-742, 770-831, 863-922, 949-986, 1023-1074, 1082-1113, 1117-1135, 1149-1176, 1191-1196, 1216, 1232-1234, 1248-1255, 1266-1267, 1276, 1295-1304, 1323-1324, 1340-1345, 1353, 1366-1369, 1377, 1382, 1391-1392, 1401, 1415-1427
sparkless/dataframe/operations/set_operations.py                       168    149    11%   28-94, 99, 121-145, 160-290, 296-316, 322-339, 344-346
sparkless/dataframe/protocols.py                                        49      0   100%
sparkless/dataframe/rdd.py                                              83     56    33%   26, 34, 42, 53, 61, 69-70, 81, 92-95, 106, 117-125, 136-143, 155-166, 174, 185, 193, 201, 209, 217, 229, 240-243, 254-261, 269, 277
sparkless/dataframe/reader.py                                          188    156    17%   66-69, 83-84, 99-100, 114-115, 129-138, 160-193, 209-262, 269-280, 284-291, 297-313, 319-354, 358-362, 366-369, 373-396, 400-404, 410-419, 423, 427, 431, 446, 461, 478-480
sparkless/dataframe/schema/__init__.py                                   3      0   100%
sparkless/dataframe/schema/operations.py                                22      9    59%   23-31, 39-41, 46, 51
sparkless/dataframe/schema/schema_manager.py                           487    365    25%   58, 81-87, 91-96, 101-116, 126, 150, 185-216, 242, 246-287, 318, 323, 326-331, 340-357, 362, 378-602, 620-637, 647-663, 668-683, 693, 695, 709, 711-712, 716-719, 722, 725, 736-746, 751-787, 791-794, 798-801, 805-823, 827-835, 842-892, 900-915, 922-928, 933-967
sparkless/dataframe/services/__init__.py                                 8      0   100%
sparkless/dataframe/services/aggregation_service.py                     99     72    27%   29-39, 69, 108, 126-154, 174-202, 223-254
sparkless/dataframe/services/assertion_service.py                       17      8    53%   25-27, 33-35, 41-43, 49-51
sparkless/dataframe/services/display_service.py                        126    102    19%   39-101, 120-167, 171-174, 182-186, 195-204, 208-223, 227-232, 238-240, 244-250, 264-267, 280, 292-297
sparkless/dataframe/services/join_service.py                           248    222    10%   42, 58-94, 114, 129, 168-443, 460, 472-496, 510-534, 549-590, 606-628
sparkless/dataframe/services/misc_service.py                           613    565     8%   56-73, 84-124, 159-282, 304-352, 373-400, 417-458, 476-560, 574-654, 669-720, 737-780, 802-817, 830-846, 865-872, 897-929, 955-1016, 1048-1126, 1153-1190, 1227-1277, 1285-1318, 1322-1340, 1354-1381, 1396-1401, 1419, 1435-1437, 1451-1458, 1469-1472, 1481, 1499-1503, 1518, 1529-1534, 1540, 1551-1554, 1562, 1567, 1576-1577, 1586, 1600-1612
sparkless/dataframe/services/schema_service.py                           4      0   100%
sparkless/dataframe/services/transformation_service.py                 346    251    27%   53-90, 140, 147, 165-181, 188-192, 202-216, 220-224, 236-237, 241-256, 261-265, 273-279, 283, 298-305, 320-450, 466-468, 489, 498-508, 523-533, 549, 585-614, 622, 630-648, 657-682, 698, 715, 734-738, 756-760, 764, 768, 796-855, 875-895
sparkless/dataframe/transformations/__init__.py                          2      2     0%   8-10
sparkless/dataframe/transformations/operations.py                      224    224     0%   8-624
sparkless/dataframe/types.py                                             8      8     0%   8-25
sparkless/dataframe/validation/__init__.py                               2      0   100%
sparkless/dataframe/validation/column_validator.py                     168     97    42%   29, 34, 36, 94-115, 137, 141-161, 164-165, 185-198, 218, 245-289, 332, 337, 343, 348, 353-360, 375, 384, 405, 412, 417-426, 444, 450-459, 468, 477, 486, 491-500, 512-561
sparkless/dataframe/validation_handler.py                               21      7    67%   42, 57-58, 75-76, 115-116
sparkless/dataframe/window_handler.py                                  315    315     0%   8-698
sparkless/dataframe/writer.py                                          393    290    26%   94-95, 114-116, 130, 145-146, 160-161, 175-176, 193-195, 206-211, 229-233, 236-239, 246-251, 259, 268, 273-361, 399-443, 470-494, 508, 520, 536, 548, 560, 572, 579-585, 589-605, 609-612, 618-627, 631-636, 639-646, 649-652, 655-671, 676-691, 695-700, 721-754, 783-866, 889-893, 897, 906-918, 943-1023
sparkless/delta.py                                                     308    254    18%   46-51, 61-74, 87-88, 100-116, 121-122, 127, 131-132, 137-152, 156-185, 189, 198, 210, 221-286, 300-341, 345, 348-352, 355-357, 360-368, 373-379, 384-408, 421-430, 434, 438, 441-446, 449-450, 455-456, 459-461, 464-466, 470-514, 517-523, 526-540, 547-551, 563-576, 581-584, 589-593, 601-629, 637-657
sparkless/error_simulation.py                                           89     89     0%   29-338
sparkless/errors.py                                                     28     10    64%   57, 62, 67, 72, 79, 86, 91, 96, 101, 106
sparkless/functions/__init__.py                                         29      6    79%   560-569
sparkless/functions/aggregate.py                                       323    205    37%   64-66, 70, 90-100, 115-123, 161-169, 184-192, 211-212, 227-228, 243-244, 259-260, 275-283, 299-300, 315-316, 331-332, 347-355, 370-371, 386-387, 402-403, 418-420, 439-443, 461-472, 490-501, 516-517, 534-535, 552-553, 570-571, 591-597, 617-623, 640-641, 658-659, 676-677, 699-716, 733-734, 751-752, 769-770, 787-788, 803-807, 825-826, 843-844, 857-859, 874-890, 904-912, 930-940, 958-967, 985-994, 1014-1026, 1044-1053, 1071-1080, 1098-1107, 1125-1134, 1152-1161, 1182-1222
sparkless/functions/array.py                                           272    185    32%   60-63, 83-88, 111-116, 139-144, 165-168, 189-192, 218-224, 250-256, 282-288, 314-320, 351-361, 390-401, 423-426, 445-448, 470-473, 491-495, 516-519, 543-546, 566-569, 584-590, 608-611, 631-634, 649-652, 667-670, 685-688, 703-706, 721-724, 742-747, 767-770, 784-787, 799-802, 816-835, 858-888, 905-908, 932-981, 1002-1005, 1023-1027, 1045-1048, 1063-1064, 1079-1091, 1103-1105, 1117-1121
sparkless/functions/base.py                                            135     79    41%   72, 87, 95, 99, 107, 109, 111, 118-121, 125, 128, 132-139, 150-161, 165-171, 175-184, 188-201, 205-217, 221-233, 237-239, 250-251, 265, 277, 281, 285, 289, 293, 297, 302, 307, 312, 317, 322
sparkless/functions/bitwise.py                                         108     67    38%   31-34, 50-53, 71, 86-89, 107-110, 125-128, 143-146, 161-168, 183-198, 213-228, 243-258, 276-283, 300-307, 324-331, 347-350, 367-370, 387-390, 405-408, 425-428
sparkless/functions/conditional.py                                     396    301    24%   33-115, 147, 152, 214, 226, 230, 234, 238, 242, 246, 251, 256, 261, 266, 271, 275, 279, 283, 295-300, 304-367, 381-384, 396-412, 426-455, 472-486, 498-502, 514-521, 533-538, 553, 570-603, 622, 637-648, 661-682, 707-722, 737-752, 767-782, 797-812, 827-842, 854-861, 873-880, 895-908, 923-939, 954-970, 985-1001
sparkless/functions/core/__init__.py                                     6      0   100%
sparkless/functions/core/column.py                                     455    236    48%   43, 53, 57, 61, 65, 73, 77, 81, 85, 89, 93, 97, 102, 108, 113, 119, 125, 131, 135, 139, 143, 147, 151, 155, 159, 163, 194, 199, 209-210, 216, 220, 224, 239, 247, 251, 255, 259, 263, 267, 285, 306, 331-345, 389, 395, 399, 418-434, 462-464, 468-470, 474-476, 480-482, 490, 498-508, 516-526, 534-544, 552-562, 570-580, 588-598, 607, 687, 694, 700, 704, 708, 711, 715, 719, 721, 723, 725, 727, 729, 731, 733, 735, 737-739, 741-743, 746, 749-755, 758, 761-763, 766, 769-771, 773-775, 777-779, 781-783, 785-787, 790-813, 816-823, 826-834, 836-840, 842-846, 848-852, 854-858, 861-883, 889, 908, 917-918, 927, 932, 956, 962, 968, 970-971, 973-989, 991-995, 1007, 1011, 1028, 1044, 1048-1050
sparkless/functions/core/expressions.py                                109     74    32%   28-32, 55, 73, 91-94, 108-115, 127-129, 141-143, 155-157, 169-171, 188-228, 240-247, 259-266, 278-285, 297-304, 320-323
sparkless/functions/core/lambda_parser.py                              146    126    14%   58-134, 142-148, 167-175, 189-247, 260-274, 285-298, 309-314, 327-332, 359-361, 369, 377, 381-385
sparkless/functions/core/literals.py                                   131     87    34%   38-54, 65-76, 81, 86, 99-101, 108-110, 117-119, 123-125, 129-131, 135-137, 141-143, 147-149, 153-155, 159-161, 165-167, 171-173, 177-179, 183-185, 189-191, 195-197, 201-203, 207-209, 213, 217, 227-229, 233-240, 244-246, 250-252, 256-258, 262-268, 272-274, 278-280, 284-286, 302, 306-308, 312-314, 318-320
sparkless/functions/core/operations.py                                 106     67    37%   27-29, 33-35, 39-41, 45-47, 51-53, 57-59, 63-65, 69, 73, 77-79, 83-85, 89-91, 95-97, 101-103, 107-109, 122, 126, 130, 134, 145, 150-154, 158, 162, 176-189, 199, 203, 211, 219-221, 225-227, 235-237
sparkless/functions/core/sql_expr_parser.py                            294    154    48%   64-74, 86-97, 145, 147-153, 159-167, 171-179, 184-200, 205-222, 244-249, 275-285, 290-296, 304-321, 327-355, 373, 375, 379, 384, 393, 402, 463, 465, 475-481, 494-525
sparkless/functions/crypto.py                                           48     39    19%   49-71, 91-113, 133-155
sparkless/functions/datetime.py                                        474    333    30%   49-52, 69-74, 86-89, 98, 107, 119-125, 137-143, 155-161, 173-179, 188-189, 205-218, 231-237, 262-301, 327-366, 390-429, 455-500, 520-549, 564-580, 595-601, 616-622, 637-646, 658-664, 676-682, 694-700, 712-718, 730-736, 748-754, 772-794, 813-844, 856-860, 872-876, 888-892, 904-908, 920-924, 936-942, 954-960, 972-978, 990-994, 1006-1010, 1022-1026, 1039-1048, 1063-1074, 1087-1093, 1106-1112, 1125-1134, 1149-1159, 1179-1191, 1211-1222, 1231-1234, 1248-1253, 1262-1273, 1283-1294, 1317-1320, 1340-1343, 1368-1372, 1393-1396, 1422-1468, 1492-1497, 1518-1537, 1557-1560, 1576-1579, 1600-1603, 1622-1627, 1646-1647, 1664-1665, 1685-1693, 1711-1727, 1745-1761
sparkless/functions/functions.py                                      1547    686    56%   67-77, 89-92, 117, 133-136, 142-162, 168-187, 193-212, 218-238, 244, 249, 254, 259, 264, 269, 274, 279, 286, 300, 307, 312, 317, 324, 329, 334, 341, 346, 351, 356, 361, 366, 371, 376, 381, 394-396, 401, 408, 415, 422, 427, 432, 437, 442, 447, 452, 457, 462, 467, 472, 483, 490, 497, 502, 507, 512, 523, 533, 543, 548, 555, 564, 569, 574, 581, 588, 593, 598, 605, 610, 615, 622, 627, 632, 637, 642, 647, 652, 657, 664, 669, 674, 680, 685, 690, 695, 700, 705, 710, 721, 726, 731, 736, 741, 748, 755, 760, 765, 770, 775, 780, 785, 790, 795, 800, 805, 810, 817, 822, 827, 832, 837, 842, 847, 852, 857, 862, 867, 872, 877, 882, 887, 892, 902, 907, 912, 917, 922, 927, 932, 937, 942, 948, 953, 963, 968, 975, 980, 985, 990, 996, 1002, 1007, 1012, 1018, 1023, 1028, 1033, 1038, 1045, 1052, 1059, 1064, 1077, 1082, 1087, 1092, 1097, 1104, 1109, 1114, 1119, 1128, 1133, 1138, 1143, 1148, 1155, 1162, 1167, 1172, 1182-1183, 1192-1193, 1202-1205, 1212, 1219, 1224, 1231, 1238, 1243, 1248, 1253, 1258, 1263, 1269, 1274, 1279, 1284, 1293-1296, 1301, 1306, 1311, 1316, 1321, 1326, 1331, 1336, 1341, 1346, 1364-1420, 1425, 1430, 1435, 1442, 1447, 1452, 1457, 1466, 1471, 1476, 1483, 1491, 1496, 1501, 1506, 1511, 1516, 1521, 1533-1538, 1549, 1556, 1563, 1569-1571, 1579-1586, 1595-1597, 1607-1617, 1626-1634, 1643-1651, 1667-1674, 1690-1697, 1706-1713, 1722-1729, 1738-1745, 1754-1761, 1770-1777, 1786-1793, 1798-1803, 1809, 1814, 1819, 1824, 1829, 1834, 1841, 1848, 1855, 1860, 1865, 1873, 1880, 1887, 1894, 1904, 1913, 1919, 1924, 1929, 1934, 1939, 1946, 1951, 1956, 1963, 1968, 1973, 1978, 1983, 1988, 1993, 2000, 2005, 2010, 2015, 2020, 2029, 2034, 2040, 2045, 2050, 2055, 2062, 2068, 2073, 2078, 2085, 2092, 2099, 2108, 2114-2138, 2152-2158, 2169, 2174, 2179, 2185, 2190, 2195, 2200, 2205, 2210, 2215, 2220, 2225, 2233, 2242-2243, 2248, 2253, 2259, 2264, 2269, 2279, 2285, 2290, 2297, 2302, 2307, 2316-2318, 2326-2328, 2336-2338, 2346-2348, 2353-2355, 2360-2362, 2369-2371, 2378-2380, 2387-2389, 2396-2398, 2404, 2409, 2414, 2419, 2424, 2429, 2434, 2439, 2444, 2449, 2454, 2464-2466, 2471-2473, 2478-2480, 2485-2487, 2492-2494, 2503-2505, 2510-2512, 2517-2519, 2525-2527, 2532-2534, 2539-2541, 2546-2548, 2553-2555, 2561-2563, 2568-2570, 2575-2577, 2582-2584, 2589-2591, 2596-2598, 2603-2605, 2634-2687, 2716-2741, 2765-2772, 2789-2791, 2797, 2804, 2809, 2814, 2821, 2828, 2833, 2842, 2847, 2854, 2861, 2866, 2871, 2879, 2884, 2891, 2898, 2905, 2913, 2920, 2927, 2933, 2938, 2945, 2950, 2962, 2975, 2989, 3004, 3016, 3023, 3030, 3037, 3044, 3049, 3054, 3059, 3064, 3069, 3074, 3080, 3085, 3090, 3097, 3102, 3107, 3112, 3117, 3122, 3130, 3135, 3140, 3149, 3155, 3160, 3165, 3170, 3175, 3196-3213
sparkless/functions/json_csv.py                                         41     41     0%   8-161
sparkless/functions/map.py                                              99     70    29%   51-54, 69-72, 87-90, 107-115, 138-143, 167-218, 239-242, 262-265, 288-294, 320-326, 352-358, 387-396, 419-426
sparkless/functions/math.py                                            320    207    35%   50-54, 66-70, 82-86, 99-105, 117-121, 133, 145-149, 161-165, 177-182, 204-231, 246-249, 264-267, 284-287, 304-307, 322-334, 349, 361-365, 377-381, 393-397, 409-414, 426-439, 451-461, 476-477, 489-490, 505-506, 518-519, 531-532, 544-545, 567-574, 586-587, 599-600, 612-613, 625-626, 638-639, 651-652, 664-665, 677-679, 696-698, 715-716, 729-730, 745-748, 768-792, 809-810, 823-824, 836-837, 849-850, 859-862, 871-874, 886-887, 902-907, 921-926, 941-960, 972, 990-1003, 1026-1067
sparkless/functions/metadata.py                                         33     33     0%   8-109
sparkless/functions/ordering.py                                         28     28     0%   7-93
sparkless/functions/pandas_types.py                                      6      0   100%
sparkless/functions/string.py                                          547    372    32%   49-53, 65-69, 81-85, 97-103, 115-121, 133-137, 149-153, 165-169, 184-196, 209-218, 231-237, 250-256, 268-274, 287-296, 309-318, 331-337, 350-356, 370-379, 395, 411-420, 435-450, 462-468, 480-484, 496, 508, 521-534, 550-561, 578-590, 606-615, 628-641, 654-667, 683-693, 705-709, 721-725, 737-741, 760-769, 791-803, 818-822, 838-844, 859-863, 878-882, 900-903, 923-926, 941-944, 960-971, 1003-1006, 1030-1033, 1054-1057, 1075-1078, 1099-1102, 1124-1127, 1146-1149, 1169-1174, 1202-1223, 1235-1238, 1250-1253, 1265-1268, 1280-1299, 1316-1334, 1352-1355, 1370-1373, 1391-1394, 1414-1417, 1432-1435, 1451-1457, 1474-1477, 1492-1495, 1508-1514, 1529-1540, 1553-1562, 1575-1584, 1601-1610, 1627-1636, 1649-1655, 1673-1684, 1698-1710, 1725-1731, 1746-1752, 1764-1768, 1781-1792, 1807, 1832-1854, 1872-1879, 1897-1902, 1918-1927, 1939-1940
sparkless/functions/udf.py                                              51     41    20%   41-46, 57-58, 70-90, 119-121, 133-155
sparkless/functions/window_execution.py                                651    602     8%   34-85, 89, 100-101, 115, 129, 143, 157, 171, 185, 201-203, 219, 235-237, 253-255, 269, 283, 297, 311, 325, 339, 350, 361, 377, 390-428, 432-471, 478-553, 557-607, 613-629, 633-694, 698-742, 746-790, 794-845, 849-916, 920-981, 987-1024, 1028-1087, 1092, 1101-1146, 1150-1230, 1234-1260, 1268-1304, 1308-1372, 1376-1420, 1424-1468
sparkless/functions/xml.py                                              65     39    40%   25-28, 50-64, 84-87, 107-110, 131-134, 155-158, 179-182, 203-206, 227-230, 251-254, 275-278
sparkless/optimizer/__init__.py                                          3      0   100%
sparkless/optimizer/optimization_rules.py                              174    144    17%   16-61, 65-66, 71, 83-110, 114-115, 119-142, 150-170, 174-175, 181-184, 192-224, 228, 233, 241-266, 270, 275-278, 286-324, 328, 333, 341-361, 365-366, 371-376
sparkless/optimizer/query_optimizer.py                                 256    185    28%   47-58, 67, 72, 80-125, 129-130, 135, 147-174, 178-179, 183-206, 214-234, 238-239, 245-248, 256-288, 292, 297, 305-330, 334, 339-342, 379-392, 403-408, 413, 418-456, 459-472, 477-504, 508, 512, 518
sparkless/performance_simulation.py                                     91     91     0%   29-329
sparkless/session/__init__.py                                            4      0   100%
sparkless/session/catalog.py                                           258    220    15%   39, 43, 47, 60-61, 65, 69, 111, 119, 130-134, 142, 150, 169-190, 211-233, 250-318, 335-352, 374, 386-414, 430-450, 462-487, 498-519, 528, 537, 546, 566-575, 605-661, 666, 683-716
sparkless/session/config/__init__.py                                     2      0   100%
sparkless/session/config/configuration.py                               54     22    59%   77, 85-86, 94, 102, 110, 118-119, 130, 141, 146, 150, 188, 199-200, 211-212, 224-225, 236-237, 245
sparkless/session/context.py                                            36     10    72%   47, 51, 55, 92, 101, 110, 119, 123, 127, 131
sparkless/session/core/__init__.py                                       4      0   100%
sparkless/session/core/builder.py                                       30     18    40%   31, 42-43, 54, 68-72, 81-99
sparkless/session/core/session.py                                      227     80    65%   12-13, 122, 174-177, 182, 187, 192, 202, 237-239, 249-254, 258-260, 273, 277, 287, 293, 306-308, 350, 368, 380, 391, 407, 414, 421-436, 444-446, 468, 470, 474, 479-482, 509-510, 525, 530-536, 544, 553-555, 575-580, 597, 604, 611, 621, 628, 633-643, 652, 661-662, 667
sparkless/session/performance_tracker.py                                39     19    51%   57, 87-109, 117
sparkless/session/services/__init__.py                                   6      0   100%
sparkless/session/services/dataframe_factory.py                        216    142    34%   14-15, 73-78, 87-99, 102-103, 106, 119, 131-170, 182, 190, 197-199, 203-211, 218, 255, 271, 281, 296-336, 343, 350, 361-364, 384-416, 428, 440, 447, 465-472, 490-506, 517-587
sparkless/session/services/lifecycle_manager.py                         21     10    52%   32-34, 42-43, 55-59
sparkless/session/services/mocking_coordinator.py                       32     21    34%   39-57, 69-72, 84-86, 102-104, 109
sparkless/session/services/protocols.py                                 20      0   100%
sparkless/session/services/sql_parameter_binder.py                      29     25    14%   28-51, 62-74
sparkless/session/session.py                                             0      0   100%
sparkless/session/sql/__init__.py                                        5      0   100%
sparkless/session/sql/executor.py                                     1280    946    26%   83-90, 124-143, 149, 179-190, 204-211, 232-239, 281, 284, 289, 310-352, 366-381, 419, 479-494, 514, 517-520, 544-545, 556-569, 605-613, 616-631, 637-643, 648-652, 654-658, 668, 681-733, 751-754, 793-820, 825-841, 851-867, 875-910, 965-1011, 1053-1061, 1081-1129, 1185-1192, 1197-1198, 1221-1243, 1268-1271, 1303, 1310, 1319-1326, 1329-1336, 1344-1348, 1363-1478, 1492-1529, 1544-1665, 1676-1705, 1720-1880, 1894-1988, 2002-2030, 2042-2088, 2100-2407, 2423-2644, 2667-2751, 2764-2779, 2806-2838, 2860-2867, 2899-2908, 2935-3001
sparkless/session/sql/optimizer.py                                      65     48    26%   41-43, 47, 51, 68, 85-100, 111-114, 127, 142, 155, 168, 181, 192-234, 247-260
sparkless/session/sql/parser.py                                        496    324    35%   263, 271-272, 290-315, 345-358, 486, 509, 540-548, 562, 564, 596-617, 668-671, 674-677, 698-833, 848-892, 906-993, 1010-1069, 1085-1106, 1121-1141, 1160-1275, 1291-1323, 1335-1338, 1360-1361
sparkless/session/sql/validation.py                                     85     72    15%   42, 105-126, 137-166, 177-188, 199-216, 227-235, 246-265, 276-279, 292, 306-311, 322-323
sparkless/spark_types.py                                               390    215    45%   60-74, 111, 115, 123, 127-151, 165, 196, 241, 256, 271, 279-281, 285, 330-346, 350, 354, 362-364, 368, 372, 384, 396, 408, 420, 432, 439-440, 443, 450-451, 454, 466, 475-477, 480, 489-491, 494, 503-505, 508, 531, 570-578, 583-590, 593, 602, 610-613, 625-633, 644-646, 654-656, 660, 664-665, 669, 673, 685, 698, 704-713, 718-733, 738-739, 746, 791, 802-805, 813-827, 835, 838-859, 863-867, 871-878, 882-889, 893, 897-922, 934-957, 962-963, 968, 974-976, 980-984, 990-991
sparkless/sql/__init__.py                                               10      0   100%
sparkless/sql/functions.py                                              30     13    57%   58-74, 84-92
sparkless/sql/types.py                                                   2      0   100%
sparkless/sql/utils.py                                                   7      0   100%
sparkless/storage/__init__.py                                           10      0   100%
sparkless/storage/backends/__init__.py                                   0      0   100%
sparkless/storage/backends/file.py                                     199    137    31%   26-34, 39, 44, 49, 53-56, 64-69, 77-78, 87-101, 112-119, 127, 135-138, 142, 146, 150, 154-155, 159-161, 174-177, 188-191, 202-203, 211-216, 224-232, 238, 242, 246, 250, 254, 258, 262-264, 268, 272, 276, 288-291, 299-300, 311, 320-327, 335, 347-349, 364-367, 376-377, 389-393, 408-413, 428-430, 442-448, 460, 470-481, 492-501, 505-509, 515-520, 527
sparkless/storage/backends/memory.py                                   133     88    34%   22-25, 34, 39, 44, 53-61, 72-77, 85, 93, 97, 101, 105, 109-110, 114-115, 127-128, 139-141, 152, 160-161, 169, 177-179, 187-188, 199, 208-209, 217, 229-231, 246-249, 258-259, 271-275, 290-295, 310-312, 324-330, 342, 352-370, 381-390, 402-406, 418-423, 430
sparkless/storage/manager.py                                           135     86    36%   40-45, 58, 70, 82, 94-98, 108, 119, 128-130, 138, 142-149, 153-159, 171, 186, 195, 208, 223, 237, 249, 258, 269, 283, 295, 303, 312, 325, 336, 344-349, 353-358, 366-378, 382, 390-400, 411-432
sparkless/storage/models.py                                             67      1    99%   77
sparkless/storage/serialization/__init__.py                              0      0   100%
sparkless/storage/serialization/csv.py                                  46     32    30%   23-30, 42-47, 57-62, 76-92, 104-120
sparkless/storage/serialization/json.py                                 39     25    36%   23-24, 36-41, 51-63, 75-90, 102-118
sparkless/utils/profiling.py                                            96     43    55%   47-48, 58, 61, 71-76, 85, 88-90, 94, 99, 102, 108, 118-135, 158-172, 182, 188
sparkless/window.py                                                     67     46    31%   57-60, 77-96, 113-132, 147-151, 166-170, 174-189, 219, 229, 234, 239
--------------------------------------------------------------------------------------------------
TOTAL                                                                32723  25427    22%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_union - sparkless.core.exceptions.execution.QueryExecutionException: Failed to execute query: Operation 'Operations: select, union' is not supported
Reason: Backend 'robin' does not support these operations
Alternative: Consider using _materialize_manual() or a different backend
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_subquery - TypeError: '>' not supported between instances of 'builtins.Column' and 'builtins.Column'
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_case_when - sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: select' is not supported
Reason: Backend 'robin' does not support these operations
Alternative: Consider using _materialize_manual() or a different backend
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_like - sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: filter' is not supported
Reason: Backend 'robin' does not support these operations
Alternative: Consider using _materialize_manual() or a different backend
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_in_clause - sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: filter' is not supported
Reason: Backend 'robin' does not support these operations
Alternative: Consider using _materialize_manual() or a different backend
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_inner_join - sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: join, select' is not supported
Reason: Backend 'robin' does not support these operations
Alternative: Consider using _materialize_manual() or a different backend
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_left_join - sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: join, select' is not supported
Reason: Backend 'robin' does not support these operations
Alternative: Consider using _materialize_manual() or a different backend
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_order_by - sparkless.core.exceptions.operation.SparkUnsupportedOperationError: Operation 'Operations: orderBy' is not supported
Reason: Backend 'robin' does not support these operations
Alternative: Consider using _materialize_manual() or a different backend
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_limit - ValueError: RobinMaterializer PoC only supports schema with exactly 3 columns
FAILED tests/parity/sql/test_advanced.py::TestSQLAdvancedParity::test_sql_with_having - TypeError: '>' not supported between instances of 'builtins.Column' and 'builtins.Column'
FAILED tests/examples/test_unified_infrastructure_example.py::TestUnifiedInfrastructure::test_comparison - AssertionError: Failed to count rows: '>' not supported between instances of 'builtins.Column' and 'builtins.Column'
FAILED tests/examples/test_unified_infrastructure_example.py::TestUnifiedInfrastructure::test_with_backend_info - AssertionError: assert 'robin' in ['mock', 'pyspark']
============================= 12 failed in 13.36s ==============================


=== run_robin_failure_sample.py ===
/Users/odosmatthews/.pyenv/versions/3.11.13/bin/python -m pytest tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_existing_table tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_new_table tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_multiple_append_operations tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_active_session tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_multiple_sessions tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_storage_manager_detached_write_visible_to_session tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_pipeline_logs_like_write_visible_immediately -v --tb=long -p no:cacheprovider

============================= test session starts ==============================
platform darwin -- Python 3.11.13, pytest-8.4.2, pluggy-1.6.0 -- /Users/odosmatthews/.pyenv/versions/3.11.13/bin/python
hypothesis profile 'ci' -> database=None, deadline=None, print_blob=True, derandomize=True, suppress_health_check=(HealthCheck.too_slow,)
rootdir: /Users/odosmatthews/Documents/coding/sparkless
configfile: pyproject.toml
plugins: anyio-4.11.0, cov-7.0.0, green-light-0.2.0, asyncio-1.2.0, xdist-3.8.0, hypothesis-6.148.7, alt-pytest-asyncio-0.9.3, async-sqlalchemy-0.2.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 7 items

tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_existing_table FAILED [ 14%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_existing_table ERROR [ 14%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_new_table FAILED [ 28%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_new_table ERROR [ 28%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_multiple_append_operations FAILED [ 42%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_multiple_append_operations ERROR [ 42%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_active_session FAILED [ 57%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_active_session ERROR [ 57%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_multiple_sessions FAILED [ 71%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_multiple_sessions ERROR [ 71%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_storage_manager_detached_write_visible_to_session FAILED [ 85%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_storage_manager_detached_write_visible_to_session ERROR [ 85%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_pipeline_logs_like_write_visible_immediately FAILED [100%]
tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_pipeline_logs_like_write_visible_immediately ERROR [100%]

==================================== ERRORS ====================================
_ ERROR at teardown of TestParquetFormatTableAppend.test_parquet_format_append_to_existing_table _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10a672d10>
spark = <sparkless.session.core.session.SparkSession object at 0x109bcf790>

    @pytest.fixture(autouse=True)
    def setup_method(self, spark):
        """Set up test environment and clean up after test."""
        self.spark = spark
        self.schema_name = "test_schema"
        self.table_name = "test_table"
        self.table_fqn = f"{self.schema_name}.{self.table_name}"
    
        # Ensure schema exists
        self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {self.schema_name}")
    
        yield
    
        # Clean up table and schema after test
        self.spark.sql(f"DROP TABLE IF EXISTS {self.table_fqn}")
        self.spark.sql(f"DROP SCHEMA IF EXISTS {self.schema_name} CASCADE")
    
        # Clean up any created parquet files manually if not in-memory
>       if self.spark._storage.db_path != ":memory:":
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'RobinStorageManager' object has no attribute 'db_path'

tests/parity/dataframe/test_parquet_format_table_append.py:35: AttributeError
_ ERROR at teardown of TestParquetFormatTableAppend.test_parquet_format_append_to_new_table _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56c750>
spark = <sparkless.session.core.session.SparkSession object at 0x109c42ed0>

    @pytest.fixture(autouse=True)
    def setup_method(self, spark):
        """Set up test environment and clean up after test."""
        self.spark = spark
        self.schema_name = "test_schema"
        self.table_name = "test_table"
        self.table_fqn = f"{self.schema_name}.{self.table_name}"
    
        # Ensure schema exists
        self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {self.schema_name}")
    
        yield
    
        # Clean up table and schema after test
        self.spark.sql(f"DROP TABLE IF EXISTS {self.table_fqn}")
        self.spark.sql(f"DROP SCHEMA IF EXISTS {self.schema_name} CASCADE")
    
        # Clean up any created parquet files manually if not in-memory
>       if self.spark._storage.db_path != ":memory:":
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'RobinStorageManager' object has no attribute 'db_path'

tests/parity/dataframe/test_parquet_format_table_append.py:35: AttributeError
_ ERROR at teardown of TestParquetFormatTableAppend.test_parquet_format_multiple_append_operations _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56cf50>
spark = <sparkless.session.core.session.SparkSession object at 0x10ce856d0>

    @pytest.fixture(autouse=True)
    def setup_method(self, spark):
        """Set up test environment and clean up after test."""
        self.spark = spark
        self.schema_name = "test_schema"
        self.table_name = "test_table"
        self.table_fqn = f"{self.schema_name}.{self.table_name}"
    
        # Ensure schema exists
        self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {self.schema_name}")
    
        yield
    
        # Clean up table and schema after test
        self.spark.sql(f"DROP TABLE IF EXISTS {self.table_fqn}")
        self.spark.sql(f"DROP SCHEMA IF EXISTS {self.schema_name} CASCADE")
    
        # Clean up any created parquet files manually if not in-memory
>       if self.spark._storage.db_path != ":memory:":
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'RobinStorageManager' object has no attribute 'db_path'

tests/parity/dataframe/test_parquet_format_table_append.py:35: AttributeError
_ ERROR at teardown of TestParquetFormatTableAppend.test_parquet_format_append_detached_df_visible_to_active_session _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56d690>
spark = <sparkless.session.core.session.SparkSession object at 0x109c0e010>

    @pytest.fixture(autouse=True)
    def setup_method(self, spark):
        """Set up test environment and clean up after test."""
        self.spark = spark
        self.schema_name = "test_schema"
        self.table_name = "test_table"
        self.table_fqn = f"{self.schema_name}.{self.table_name}"
    
        # Ensure schema exists
        self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {self.schema_name}")
    
        yield
    
        # Clean up table and schema after test
        self.spark.sql(f"DROP TABLE IF EXISTS {self.table_fqn}")
        self.spark.sql(f"DROP SCHEMA IF EXISTS {self.schema_name} CASCADE")
    
        # Clean up any created parquet files manually if not in-memory
>       if self.spark._storage.db_path != ":memory:":
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'RobinStorageManager' object has no attribute 'db_path'

tests/parity/dataframe/test_parquet_format_table_append.py:35: AttributeError
_ ERROR at teardown of TestParquetFormatTableAppend.test_parquet_format_append_detached_df_visible_to_multiple_sessions _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56dd90>
spark = <sparkless.session.core.session.SparkSession object at 0x109c16e50>

    @pytest.fixture(autouse=True)
    def setup_method(self, spark):
        """Set up test environment and clean up after test."""
        self.spark = spark
        self.schema_name = "test_schema"
        self.table_name = "test_table"
        self.table_fqn = f"{self.schema_name}.{self.table_name}"
    
        # Ensure schema exists
        self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {self.schema_name}")
    
        yield
    
        # Clean up table and schema after test
        self.spark.sql(f"DROP TABLE IF EXISTS {self.table_fqn}")
        self.spark.sql(f"DROP SCHEMA IF EXISTS {self.schema_name} CASCADE")
    
        # Clean up any created parquet files manually if not in-memory
>       if self.spark._storage.db_path != ":memory:":
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'RobinStorageManager' object has no attribute 'db_path'

tests/parity/dataframe/test_parquet_format_table_append.py:35: AttributeError
_ ERROR at teardown of TestParquetFormatTableAppend.test_storage_manager_detached_write_visible_to_session _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56e490>
spark = <sparkless.session.core.session.SparkSession object at 0x109c2ab50>

    @pytest.fixture(autouse=True)
    def setup_method(self, spark):
        """Set up test environment and clean up after test."""
        self.spark = spark
        self.schema_name = "test_schema"
        self.table_name = "test_table"
        self.table_fqn = f"{self.schema_name}.{self.table_name}"
    
        # Ensure schema exists
        self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {self.schema_name}")
    
        yield
    
        # Clean up table and schema after test
        self.spark.sql(f"DROP TABLE IF EXISTS {self.table_fqn}")
        self.spark.sql(f"DROP SCHEMA IF EXISTS {self.schema_name} CASCADE")
    
        # Clean up any created parquet files manually if not in-memory
>       if self.spark._storage.db_path != ":memory:":
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'RobinStorageManager' object has no attribute 'db_path'

tests/parity/dataframe/test_parquet_format_table_append.py:35: AttributeError
_ ERROR at teardown of TestParquetFormatTableAppend.test_pipeline_logs_like_write_visible_immediately _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56eb90>
spark = <sparkless.session.core.session.SparkSession object at 0x109c43950>

    @pytest.fixture(autouse=True)
    def setup_method(self, spark):
        """Set up test environment and clean up after test."""
        self.spark = spark
        self.schema_name = "test_schema"
        self.table_name = "test_table"
        self.table_fqn = f"{self.schema_name}.{self.table_name}"
    
        # Ensure schema exists
        self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS {self.schema_name}")
    
        yield
    
        # Clean up table and schema after test
        self.spark.sql(f"DROP TABLE IF EXISTS {self.table_fqn}")
        self.spark.sql(f"DROP SCHEMA IF EXISTS {self.schema_name} CASCADE")
    
        # Clean up any created parquet files manually if not in-memory
>       if self.spark._storage.db_path != ":memory:":
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'RobinStorageManager' object has no attribute 'db_path'

tests/parity/dataframe/test_parquet_format_table_append.py:35: AttributeError
=================================== FAILURES ===================================
__ TestParquetFormatTableAppend.test_parquet_format_append_to_existing_table ___

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10a672d10>
spark = <sparkless.session.core.session.SparkSession object at 0x109bcf790>

    def test_parquet_format_append_to_existing_table(self, spark):
        """Test that data appended to an existing parquet format table is immediately visible."""
        schema = StructType(
            [
                StructField("id", IntegerType(), True),
                StructField("name", StringType(), True),
            ]
        )
    
        # Create empty table with parquet format (like LogWriter does)
        empty_df = spark.createDataFrame([], schema)
        empty_df.write.format("parquet").mode("overwrite").saveAsTable(self.table_fqn)
    
        # Verify initial state
        result1 = spark.table(self.table_fqn)
        assert result1.count() == 0, "Initial table should be empty"
    
        # Append data with parquet format
        data1 = [{"id": 1, "name": "test1"}]
        df1 = spark.createDataFrame(data1, schema)
        df1.write.format("parquet").mode("append").saveAsTable(self.table_fqn)
    
        # Verify appended data is immediately visible
        result2 = spark.table(self.table_fqn)
        count = result2.count()
>       assert count == 1, (
            f"Table should have 1 row after append, got {count}. "
            "This verifies fix for issue #114."
        )
E       AssertionError: Table should have 1 row after append, got 2. This verifies fix for issue #114.
E       assert 2 == 1

tests/parity/dataframe/test_parquet_format_table_append.py:72: AssertionError
_____ TestParquetFormatTableAppend.test_parquet_format_append_to_new_table _____

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56c750>
spark = <sparkless.session.core.session.SparkSession object at 0x109c42ed0>

    def test_parquet_format_append_to_new_table(self, spark):
        """Test that appending to a non-existent parquet format table creates it correctly."""
        schema = StructType(
            [
                StructField("id", IntegerType(), True),
                StructField("name", StringType(), True),
            ]
        )
    
        # Append to non-existent table with parquet format (should create it)
        data1 = [{"id": 1, "name": "initial"}]
        df1 = spark.createDataFrame(data1, schema)
        df1.write.format("parquet").mode("append").saveAsTable(self.table_fqn)
    
        # Verify data is immediately visible
        result = spark.table(self.table_fqn)
        count = result.count()
>       assert count == 1, f"New table created by append should have 1 row, got {count}"
E       AssertionError: New table created by append should have 1 row, got 2
E       assert 2 == 1

tests/parity/dataframe/test_parquet_format_table_append.py:108: AssertionError
_ TestParquetFormatTableAppend.test_parquet_format_multiple_append_operations __

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56cf50>
spark = <sparkless.session.core.session.SparkSession object at 0x10ce856d0>

    def test_parquet_format_multiple_append_operations(self, spark):
        """Test that multiple parquet format append operations preserve all data."""
        schema = StructType(
            [
                StructField("id", IntegerType(), True),
                StructField("value", StringType(), True),
            ]
        )
    
        # Create initial table with parquet format
        data1 = [{"id": 1, "value": "a"}]
        df1 = spark.createDataFrame(data1, schema)
        df1.write.format("parquet").mode("overwrite").saveAsTable(self.table_fqn)
    
        # Perform multiple append operations with parquet format
        for i in range(2, 6):
            data = [{"id": i, "value": chr(ord("a") + i - 1)}]
            df = spark.createDataFrame(data, schema)
            df.write.format("parquet").mode("append").saveAsTable(self.table_fqn)
    
            # Verify data is visible after each append
            result = spark.table(self.table_fqn)
>           assert result.count() == i, (
                f"After {i - 1} appends, table should have {i} rows, got {result.count()}"
            )
E           AssertionError: After 1 appends, table should have 2 rows, got 4
E           assert 4 == 2
E            +  where 4 = count()
E            +    where count = DataFrame[4 rows, 2 columns].count

tests/parity/dataframe/test_parquet_format_table_append.py:147: AssertionError
_ TestParquetFormatTableAppend.test_parquet_format_append_detached_df_visible_to_active_session _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56d690>
spark = <sparkless.session.core.session.SparkSession object at 0x109c0e010>

    def test_parquet_format_append_detached_df_visible_to_active_session(self, spark):
        """Detached DataFrame writes should be immediately visible to the active session."""
        schema = StructType(
            [
                StructField("id", IntegerType(), True),
                StructField("name", StringType(), True),
            ]
        )
    
        # Construct DataFrame without using the session (no shared storage)
        from sparkless.dataframe import DataFrame
    
        detached_df = DataFrame(
            [
                {"id": 1, "name": "alpha"},
                {"id": 2, "name": "beta"},
            ],
            schema,
        )
    
        # Write via detached DataFrame; should sync into the active session's storage
        detached_df.write.format("parquet").mode("append").saveAsTable(self.table_fqn)
    
        result = spark.table(self.table_fqn)
>       assert result.count() == 2
E       assert 4 == 2
E        +  where 4 = count()
E        +    where count = DataFrame[4 rows, 2 columns].count

tests/parity/dataframe/test_parquet_format_table_append.py:182: AssertionError
_ TestParquetFormatTableAppend.test_parquet_format_append_detached_df_visible_to_multiple_sessions _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56dd90>
spark = <sparkless.session.core.session.SparkSession object at 0x109c16e50>

    def test_parquet_format_append_detached_df_visible_to_multiple_sessions(
        self, spark
    ):
        """Detached DataFrame writes should sync to all active sessions."""
        schema = StructType(
            [
                StructField("id", IntegerType(), True),
                StructField("name", StringType(), True),
            ]
        )
    
        # Create a second active session to verify synchronization
        spark2 = SparkSession("second_session")
    
        from sparkless.dataframe import DataFrame
    
        detached_df = DataFrame(
            [
                {"id": 1, "name": "gamma"},
                {"id": 2, "name": "delta"},
            ],
            schema,
        )
    
        detached_df.write.format("parquet").mode("append").saveAsTable(self.table_fqn)
    
        # Visible in the original session
        result1 = spark.table(self.table_fqn)
>       assert result1.count() == 2
E       assert 10 == 2
E        +  where 10 = count()
E        +    where count = DataFrame[10 rows, 2 columns].count

tests/parity/dataframe/test_parquet_format_table_append.py:214: AssertionError
_ TestParquetFormatTableAppend.test_storage_manager_detached_write_visible_to_session _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56e490>
spark = <sparkless.session.core.session.SparkSession object at 0x109c2ab50>

    def test_storage_manager_detached_write_visible_to_session(self, spark):
        """Writes via a standalone PolarsStorageManager should surface in the active session."""
        schema = StructType(
            [
                StructField("id", IntegerType(), True),
                StructField("name", StringType(), True),
            ]
        )
    
        storage = PolarsStorageManager()
        storage.create_schema(self.schema_name)
        storage.create_table(self.schema_name, self.table_name, schema.fields)
    
        data = [{"id": 1, "name": "omega"}, {"id": 2, "name": "sigma"}]
        storage.insert_data(self.schema_name, self.table_name, data)
    
        result = spark.table(self.table_fqn)
>       assert result.count() == 2
E       assert 10 == 2
E        +  where 10 = count()
E        +    where count = DataFrame[10 rows, 2 columns].count

tests/parity/dataframe/test_parquet_format_table_append.py:246: AssertionError
_ TestParquetFormatTableAppend.test_pipeline_logs_like_write_visible_immediately _

self = <tests.parity.dataframe.test_parquet_format_table_append.TestParquetFormatTableAppend object at 0x10b56eb90>
spark = <sparkless.session.core.session.SparkSession object at 0x109c43950>

    def test_pipeline_logs_like_write_visible_immediately(self, spark):
        """Simulate pipeline_logs writes: append to new table then read back immediately."""
        schema = StructType(
            [
                StructField("run_id", StringType(), True),
                StructField("step", StringType(), True),
                StructField("status", StringType(), True),
            ]
        )
    
        rows = [
            {"run_id": "run1", "step": "pipeline", "status": "success"},
            {"run_id": "run1", "step": "extract", "status": "success"},
            {"run_id": "run1", "step": "transform", "status": "success"},
            {"run_id": "run1", "step": "load", "status": "success"},
        ]
    
        df = spark.createDataFrame(rows, schema)
        df.write.format("parquet").mode("append").saveAsTable(self.table_fqn)
    
        result = spark.table(self.table_fqn)
>       assert result.count() == 4, (
            "pipeline_logs table should return all appended rows"
        )
E       AssertionError: pipeline_logs table should return all appended rows
E       assert 20 == 4
E        +  where 20 = count()
E        +    where count = DataFrame[20 rows, 3 columns].count

tests/parity/dataframe/test_parquet_format_table_append.py:271: AssertionError
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.11.13-final-0 _______________

Name                                                                 Stmts   Miss  Cover   Missing
--------------------------------------------------------------------------------------------------
sparkless/__init__.py                                                   24      0   100%
sparkless/_version.py                                                    8      4    50%   10-12, 16-19
sparkless/backend/__init__.py                                            3      0   100%
sparkless/backend/factory.py                                           131     82    37%   51-53, 55-63, 72-74, 76-79, 82, 90, 113-147, 162-196, 212-238, 249, 266, 280-281, 296-297
sparkless/backend/polars/__init__.py                                     4      0   100%
sparkless/backend/polars/_over_compat.py                                19     12    37%   21-27, 36-40
sparkless/backend/polars/executors/__init__.py                           1      1     0%   13
sparkless/backend/polars/export.py                                      36     28    22%   27-53, 67-76, 88-89, 102-103, 113-114
sparkless/backend/polars/expression_translator.py                     2118   2069     2%   40-48, 59-148, 161-167, 178-192, 201-214, 236-308, 327-362, 378-383, 395-399, 417-1011, 1033-1170, 1201-1277, 1289-1499, 1514, 1519-1523, 1526-1565, 1568-1581, 1584-1588, 1591-1595, 1603-1604, 1625-4438, 4450-4467, 4480-4490, 4503-4529, 4542-4596, 4607-4653, 4666-4694, 4705-4729, 4742-4897, 4908-4957, 4969-4976, 4988-5022, 5035-5068, 5083-5093
sparkless/backend/polars/materializer.py                               813    790     3%   48-49, 58-68, 75-105, 125-1723, 1735-1750, 1766-1781, 1793-1800, 1815-1850, 1865-1871, 1880-1882, 1887
sparkless/backend/polars/operation_executor.py                        2032   1981     3%   37-42, 56-113, 121-130, 139-149, 164-170, 176-177, 191-347, 360-1983, 1992-2000, 2005-2020, 2025-2046, 2051-2055, 2060-2068, 2071-2084, 2087-2102, 2105-2108, 2113-2125, 2128-2147, 2169-3372, 3388-3434, 3466-3610, 3633-3904, 3934-4223, 4241-4313, 4326-4347, 4362-4420, 4432, 4444, 4461-4484, 4495, 4507, 4522
sparkless/backend/polars/parquet_storage.py                             41     41     0%   8-119
sparkless/backend/polars/plan_interpreter.py                           381    381     0%   8-531
sparkless/backend/polars/schema_registry.py                             73     55    25%   42-43, 59-94, 110-190, 203-207, 220-222
sparkless/backend/polars/schema_utils.py                                31      7    77%   19, 31-38, 53
sparkless/backend/polars/storage.py                                    350    166    53%   61, 68-69, 82-83, 87-94, 98-103, 108, 118, 149-152, 170-171, 183-185, 189-196, 206, 225-226, 242, 274, 286, 289-290, 302, 306-307, 311-315, 319, 325-349, 366, 371, 375-377, 385-400, 420, 441, 453-457, 477-481, 494-538, 553, 561, 574, 578, 607, 611, 626-634, 669, 681-690, 702-703, 707, 714, 718, 724, 735, 740-743, 753-771
sparkless/backend/polars/translators/__init__.py                         1      1     0%   14
sparkless/backend/polars/translators/arithmetic_translator.py           17     17     0%   8-70
sparkless/backend/polars/translators/string_translator.py              145    145     0%   8-302
sparkless/backend/polars/translators/type_translator.py                107    107     0%   8-190
sparkless/backend/polars/type_mapper.py                                 97     89     8%   47-92, 107-169
sparkless/backend/polars/window_handler.py                             290    278     4%   20-25, 30-44, 56-70, 80-85, 100-113, 132-563
sparkless/backend/protocols.py                                          14      0   100%
sparkless/backend/robin/__init__.py                                      4      0   100%
sparkless/backend/robin/export.py                                       15      7    53%   18-20, 23, 26, 31, 40, 43
sparkless/backend/robin/materializer.py                                112     97    13%   22-23, 27, 37-76, 89, 92, 95-105, 110-114, 122-175, 178
sparkless/backend/robin/storage.py                                      43      9    79%   35, 52, 67, 72, 75, 78, 84, 89, 94
sparkless/compat/__init__.py                                             2      0   100%
sparkless/compat/datetime.py                                            74     53    28%   43, 55-57, 61-67, 73-81, 96-103, 115-122, 128-133, 141-146, 162-182
sparkless/config.py                                                     55     32    42%   40-72, 76-80, 86-91, 97-99, 105
sparkless/core/__init__.py                                               9      0   100%
sparkless/core/column_resolver.py                                       33     18    45%   48, 66-73, 105-116, 138-142
sparkless/core/condition_evaluator.py                                  764    730     4%   27-34, 49-56, 71-245, 260-541, 556-576, 591-604, 619-781, 795-1203, 1218-1229, 1245-1274, 1290-1311, 1324-1340, 1353, 1366-1370, 1384-1403
sparkless/core/data_validation.py                                       80     47    41%   56-85, 91-113, 129, 136-137, 161, 168, 173-187, 205-206, 220-221
sparkless/core/ddl_adapter.py                                           31     23    26%   52-53, 65-66, 78-79, 91-106, 118-133
sparkless/core/exceptions/__init__.py                                    6      0   100%
sparkless/core/exceptions/analysis.py                                  100     81    19%   37-63, 70-73, 91, 109, 137-162, 170-202, 230-245, 269-272, 302-323
sparkless/core/exceptions/base.py                                       22      5    77%   28, 31, 61, 76, 91
sparkless/core/exceptions/execution.py                                  29     13    55%   27, 45, 63, 87-90, 114-117, 135, 153
sparkless/core/exceptions/operation.py                                  87     64    26%   24-34, 49-58, 72-82, 96-106, 120-129, 152, 173-183, 197-206
sparkless/core/exceptions/py4j_compat.py                                10     10     0%   8-41
sparkless/core/exceptions/runtime.py                                    41     25    39%   27, 53-57, 75, 101-107, 131-134, 158-161, 187-193
sparkless/core/exceptions/validation.py                                 44     26    41%   27, 45, 63, 81, 107-111, 141-147, 171-174, 202-207
sparkless/core/interfaces/__init__.py                                    5      0   100%
sparkless/core/interfaces/dataframe.py                                 180     56    69%   20, 26, 31, 36, 41, 46, 51, 56, 61, 66, 71, 76, 81, 86, 91, 98, 103, 110, 115, 120, 125, 130, 136, 142, 151, 156, 161, 166, 171, 176, 181, 190, 195, 200, 205, 212, 217, 222, 227, 232, 241, 246, 251, 256, 261, 266, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320
sparkless/core/interfaces/functions.py                                 186     56    70%   23, 28, 33, 42, 47, 56, 61, 70, 75, 84, 89, 94, 99, 108, 113, 118, 123, 130, 135, 144, 149, 154, 159, 164, 173, 178, 185, 192, 197, 202, 207, 212, 221, 226, 231, 236, 241, 250, 255, 263, 273, 278, 283, 288, 293, 298, 303, 308, 313, 318, 323, 328, 333, 338, 343, 348
sparkless/core/interfaces/session.py                                   117     34    71%   20, 26, 32, 38, 44, 50, 59, 64, 69, 76, 81, 86, 96, 101, 106, 115, 120, 125, 136, 141, 146, 151, 156, 161, 166, 171, 176, 185, 190, 195, 200, 209, 214, 219
sparkless/core/interfaces/storage.py                                   126     38    70%   29, 34, 39, 44, 54, 59, 64, 69, 76, 83, 90, 97, 102, 106, 110, 114, 120, 126, 136, 142, 148, 153, 158, 163, 168, 173, 186, 192, 198, 204, 210, 215, 220, 229, 234, 239, 248, 253
sparkless/core/protocols.py                                             42      0   100%
sparkless/core/safe_evaluator.py                                       125    116     7%   39-50, 63-67, 83-215
sparkless/core/schema_inference.py                                     107     92    14%   72-134, 157-200, 218-237, 242-251, 256-265, 279-280, 296-309
sparkless/core/type_utils.py                                            86     66    23%   20-21, 49-51, 63-65, 77-79, 91-93, 112-121, 140-144, 164-168, 187-199, 223-259, 278, 290-292, 304
sparkless/core/types/__init__.py                                         4      0   100%
sparkless/core/types/data_types.py                                     139     38    73%   22, 27, 32, 37, 42, 47, 52, 57, 66, 76, 81, 91, 97, 102, 111, 121, 127, 132, 141, 146, 155, 160, 170, 175, 180, 190, 196, 201, 206, 211, 221, 226, 231, 240, 249, 254, 259, 264
sparkless/core/types/metadata.py                                       166     47    72%   18, 23, 28, 33, 38, 43, 48, 53, 58, 63, 73, 79, 85, 91, 97, 103, 108, 113, 118, 128, 134, 140, 145, 150, 160, 166, 172, 177, 182, 187, 197, 203, 209, 215, 220, 225, 230, 239, 244, 249, 254, 259, 268, 273, 278, 283, 288
sparkless/core/types/schema.py                                         121     36    70%   22, 28, 34, 40, 45, 50, 55, 60, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 185, 190, 195, 200, 209, 214, 221, 226
sparkless/data_generation/__init__.py                                    4      4     0%   8-12
sparkless/data_generation/builder.py                                    28     28     0%   8-85
sparkless/data_generation/convenience.py                                 9      9     0%   8-64
sparkless/data_generation/generator.py                                 174    174     0%   8-337
sparkless/dataframe/__init__.py                                          6      0   100%
sparkless/dataframe/aggregations/__init__.py                             2      2     0%   8-10
sparkless/dataframe/aggregations/operations.py                          75     75     0%   8-208
sparkless/dataframe/assertions/__init__.py                               3      3     0%   7-10
sparkless/dataframe/assertions/assertions.py                            26     26     0%   8-88
sparkless/dataframe/assertions/operations.py                            16     16     0%   8-45
sparkless/dataframe/attribute_handler.py                                62     32    48%   30, 54, 77-85, 116-125, 176, 190-196, 207-209, 213-217, 223-224
sparkless/dataframe/casting/__init__.py                                  2      2     0%   3-5
sparkless/dataframe/casting/type_converter.py                           95     95     0%   3-180
sparkless/dataframe/collection_handler.py                               29     17    41%   27-29, 35-41, 47-53, 68-72, 81
sparkless/dataframe/condition_handler.py                                52     52     0%   8-180
sparkless/dataframe/dataframe.py                                       513    298    42%   179-183, 187-191, 203-207, 211-215, 219-224, 240-243, 262-265, 276, 280, 293, 305, 311, 317, 321, 325, 329, 333, 339, 345, 359, 365, 369, 373, 377, 381, 390, 394, 404, 408, 412, 420, 424, 428, 432, 436, 440, 445, 449, 453, 457, 463, 469-477, 486, 490, 496-500, 507-508, 518-532, 551, 555-561, 565, 569, 575-581, 596-609, 621, 625, 630, 634, 638, 642, 652, 674, 689-691, 700, 709, 715, 719, 723, 727, 733, 742, 746, 750, 756, 760, 770, 780, 791, 795, 799, 803, 809, 813, 817, 821, 829, 835, 839, 843, 847, 852, 856, 860, 864, 875, 888, 894, 899, 911-912, 924-929, 955-969, 987-1062, 1078-1097, 1103, 1114-1128, 1139-1174, 1184-1188, 1195, 1201-1206, 1210, 1216, 1225, 1241-1244, 1250, 1258, 1269, 1284, 1292, 1303, 1311, 1323, 1329, 1335, 1342, 1347-1353, 1368-1379, 1383, 1403-1411, 1420-1460
sparkless/dataframe/display/__init__.py                                  3      3     0%   3-6
sparkless/dataframe/display/formatter.py                                37     37     0%   3-57
sparkless/dataframe/display/operations.py                              118    118     0%   8-274
sparkless/dataframe/evaluation/__init__.py                               2      0   100%
sparkless/dataframe/evaluation/evaluators/__init__.py                    1      1     0%   13
sparkless/dataframe/evaluation/evaluators/conditional_evaluator.py      22     22     0%   7-63
sparkless/dataframe/evaluation/expression_evaluator.py                2600   2365     9%   70-78, 89-113, 117-122, 128-130, 134, 138-181, 193-259, 269-300, 306-416, 422-448, 458-1001, 1007-1046, 1052-1237, 1243-1320, 1327-1365, 1371-1422, 1428-1514, 1519-1535, 1542-1554, 1558-1573, 1577-1592, 1596-1611, 1615-1630, 1641-1646, 1650-1673, 1677, 1681, 1690-1711, 1715, 1924, 1928, 1932-1936, 1940-1944, 1948-1952, 1956-1969, 1973-1980, 1984-1994, 1998-2008, 2012-2014, 2020-2023, 2027-2030, 2034-2041, 2045-2050, 2065-2123, 2127-2132, 2143-2187, 2191-2202, 2206-2214, 2220-2222, 2226-2231, 2235-2237, 2241-2243, 2249, 2253-2259, 2263, 2267-2272, 2276-2278, 2282-2284, 2288, 2292, 2296, 2304, 2308, 2312-2315, 2319-2321, 2325-2330, 2334-2337, 2341-2353, 2360, 2364-2396, 2400-2405, 2409-2431, 2435-2457, 2463-2479, 2483-2514, 2520-2555, 2561-2568, 2572-2592, 2596-2639, 2643-2660, 2664-2677, 2682, 2686-2687, 2691, 2695, 2699, 2705-2714, 2718-2727, 2731-2738, 2744, 2748-2762, 2766-2773, 2777-2791, 2795-2802, 2806-2813, 2817-2824, 2828-2837, 2841-2850, 2854-2863, 2867-2874, 2878-2885, 2889-2896, 2900-2907, 2911-2918, 2924, 2928-2937, 2942, 2947, 2952-2984, 2988-2997, 3003, 3009, 3013-3024, 3030, 3037, 3042, 3048, 3053-3077, 3082, 3087, 3092, 3101-3133, 3137-3181, 3185-3229, 3233-3260, 3265-3286, 3294-3356, 3360-3367, 3371-3391, 3397, 3403, 3407-3522, 3527-3538, 3542-3551, 3555, 3559, 3563, 3567, 3571, 3575, 3579, 3583-3588, 3592-3598, 3602-3607, 3611-3616, 3625, 3639-3673, 3679, 3683-3690, 3694-3705, 3710-3719, 3725-3734, 3740-3745, 3751-3756, 3762-3770, 3776-3784, 3788, 3792-3798, 3802-3813, 3817-3820, 3824-3830, 3834-3836, 3841, 3846-3858, 3862, 3866-3872, 3876-3882, 3888-3898, 3902-3909, 3914, 3918-3920, 3924-3947, 3951-3974, 3979, 3984, 3989, 3994, 3999, 4004, 4008-4020, 4026-4033, 4039-4046, 4050-4059, 4065-4072, 4078-4085, 4091-4098, 4106-4120, 4124-4131, 4135-4142, 4147-4162, 4166-4181, 4201, 4206, 4211, 4217-4228, 4235-4238, 4244-4247, 4253-4258, 4263-4267, 4271-4288, 4294-4309, 4315-4330, 4336-4339, 4343-4345, 4350-4358, 4364-4391, 4396-4405, 4409-4411, 4415-4417, 4422-4441, 4445-4462, 4466-4483, 4487-4504, 4508-4513, 4519-4530, 4534-4545, 4549-4554
sparkless/dataframe/export.py                                           15     15     0%   8-46
sparkless/dataframe/grouped/__init__.py                                  5      0   100%
sparkless/dataframe/grouped/base.py                                   1053   1023     3%   56-58, 72-696, 710-765, 779-1648, 1665-1935, 1946-1953, 1964-1971, 1986, 1997-2008, 2019-2026, 2037-2044, 2055-2064, 2075-2084, 2095-2104, 2115-2124, 2135-2144, 2155-2164, 2175-2202, 2213-2240, 2254-2282, 2304-2371, 2393-2448
sparkless/dataframe/grouped/cube.py                                     86     77    10%   30-31, 47-215
sparkless/dataframe/grouped/pivot.py                                   482    459     5%   36-39, 55-356, 362-397, 403-633, 641-802, 814-823, 834-843, 854, 865-874, 885-894, 905-914, 925-931, 942-948, 959-965, 976-982, 993-999, 1010-1016, 1027-1033
sparkless/dataframe/grouped/rollup.py                                   87     79     9%   29-30, 48-220
sparkless/dataframe/joins/__init__.py                                    2      2     0%   8-10
sparkless/dataframe/joins/operations.py                                146    146     0%   8-396
sparkless/dataframe/lazy.py                                           1140   1140     0%   8-2730
sparkless/dataframe/logical_plan.py                                    221    221     0%   9-452
sparkless/dataframe/operations/__init__.py                               2      2     0%   8-10
sparkless/dataframe/operations/aggregation_operations.py               128    128     0%   3-313
sparkless/dataframe/operations/join_operations.py                      157    157     0%   3-329
sparkless/dataframe/operations/misc.py                                 516    516     0%   8-1427
sparkless/dataframe/operations/set_operations.py                       168    168     0%   3-346
sparkless/dataframe/protocols.py                                        49      0   100%
sparkless/dataframe/rdd.py                                              83     56    33%   26, 34, 42, 53, 61, 69-70, 81, 92-95, 106, 117-125, 136-143, 155-166, 174, 185, 193, 201, 209, 217, 229, 240-243, 254-261, 269, 277
sparkless/dataframe/reader.py                                          188    156    17%   66-69, 83-84, 99-100, 114-115, 129-138, 160-193, 209-262, 269-280, 284-291, 297-313, 319-354, 358-362, 366-369, 373-396, 400-404, 410-419, 423, 427, 431, 446, 461, 478-480
sparkless/dataframe/schema/__init__.py                                   3      3     0%   3-6
sparkless/dataframe/schema/operations.py                                22     22     0%   8-51
sparkless/dataframe/schema/schema_manager.py                           487    487     0%   3-967
sparkless/dataframe/services/__init__.py                                 8      0   100%
sparkless/dataframe/services/aggregation_service.py                     99     86    13%   26-39, 67-92, 108, 126-154, 174-202, 223-254
sparkless/dataframe/services/assertion_service.py                       17      8    53%   25-27, 33-35, 41-43, 49-51
sparkless/dataframe/services/display_service.py                        126    102    19%   39-101, 120-167, 171-174, 182-186, 195-204, 208-223, 227-232, 238-240, 244-250, 264-267, 280, 292-297
sparkless/dataframe/services/join_service.py                           248    232     6%   41-44, 58-94, 104-135, 168-443, 460, 472-496, 510-534, 549-590, 606-628
sparkless/dataframe/services/misc_service.py                           613    565     8%   56-73, 84-124, 159-282, 304-352, 373-400, 417-458, 476-560, 574-654, 669-720, 737-780, 802-817, 830-846, 865-872, 897-929, 955-1016, 1048-1126, 1153-1190, 1227-1277, 1285-1318, 1322-1340, 1354-1381, 1396-1401, 1419, 1435-1437, 1451-1458, 1469-1472, 1481, 1499-1503, 1518, 1529-1534, 1540, 1551-1554, 1562, 1567, 1576-1577, 1586, 1600-1612
sparkless/dataframe/services/schema_service.py                           4      0   100%
sparkless/dataframe/services/transformation_service.py                 346    310    10%   38-40, 53-90, 139-310, 320-450, 465-473, 489, 498-508, 523-533, 542-571, 585-614, 622, 630-648, 657-682, 698, 714-717, 734-738, 742, 756-760, 764, 768, 796-855, 875-895
sparkless/dataframe/transformations/__init__.py                          2      2     0%   8-10
sparkless/dataframe/transformations/operations.py                      224    224     0%   8-624
sparkless/dataframe/types.py                                             8      8     0%   8-25
sparkless/dataframe/validation/__init__.py                               2      2     0%   7-9
sparkless/dataframe/validation/column_validator.py                     168    168     0%   9-561
sparkless/dataframe/validation_handler.py                               21     21     0%   9-116
sparkless/dataframe/window_handler.py                                  315    315     0%   8-698
sparkless/dataframe/writer.py                                          393    268    32%   114-116, 130, 145-146, 160-161, 175-176, 193-195, 209-211, 229-233, 236-239, 246-251, 259, 268, 273-274, 281-284, 289, 295-297, 309-361, 399-443, 470-494, 508, 520, 536, 548, 560, 572, 579-585, 589-605, 609-612, 618-627, 631-636, 639-646, 649-652, 655-671, 676-691, 695-700, 721-754, 783-866, 889-893, 897, 907, 913-918, 943-1023
sparkless/delta.py                                                     308    254    18%   46-51, 61-74, 87-88, 100-116, 121-122, 127, 131-132, 137-152, 156-185, 189, 198, 210, 221-286, 300-341, 345, 348-352, 355-357, 360-368, 373-379, 384-408, 421-430, 434, 438, 441-446, 449-450, 455-456, 459-461, 464-466, 470-514, 517-523, 526-540, 547-551, 563-576, 581-584, 589-593, 601-629, 637-657
sparkless/error_simulation.py                                           89     89     0%   29-338
sparkless/errors.py                                                     28     10    64%   57, 62, 67, 72, 79, 86, 91, 96, 101, 106
sparkless/functions/__init__.py                                         29      6    79%   560-569
sparkless/functions/aggregate.py                                       323    215    33%   56-70, 90-100, 115-123, 138-146, 161-169, 184-192, 211-212, 227-228, 243-244, 259-260, 275-283, 299-300, 315-316, 331-332, 347-355, 370-371, 386-387, 402-403, 418-420, 439-443, 461-472, 490-501, 516-517, 534-535, 552-553, 570-571, 591-597, 617-623, 640-641, 658-659, 676-677, 699-716, 733-734, 751-752, 769-770, 787-788, 803-807, 825-826, 843-844, 857-859, 874-890, 904-912, 930-940, 958-967, 985-994, 1014-1026, 1044-1053, 1071-1080, 1098-1107, 1125-1134, 1152-1161, 1182-1222
sparkless/functions/array.py                                           272    185    32%   60-63, 83-88, 111-116, 139-144, 165-168, 189-192, 218-224, 250-256, 282-288, 314-320, 351-361, 390-401, 423-426, 445-448, 470-473, 491-495, 516-519, 543-546, 566-569, 584-590, 608-611, 631-634, 649-652, 667-670, 685-688, 703-706, 721-724, 742-747, 767-770, 784-787, 799-802, 816-835, 858-888, 905-908, 932-981, 1002-1005, 1023-1027, 1045-1048, 1063-1064, 1079-1091, 1103-1105, 1117-1121
sparkless/functions/base.py                                            135    103    24%   57-67, 71-89, 94-99, 106-139, 150-161, 165-171, 175-184, 188-201, 205-217, 221-233, 237-239, 250-251, 265, 277, 281, 285, 289, 293, 297, 302, 307, 312, 317, 322
sparkless/functions/bitwise.py                                         108     67    38%   31-34, 50-53, 71, 86-89, 107-110, 125-128, 143-146, 161-168, 183-198, 213-228, 243-258, 276-283, 300-307, 324-331, 347-350, 367-370, 387-390, 405-408, 425-428
sparkless/functions/conditional.py                                     396    320    19%   33-115, 133-142, 147, 152, 164-165, 176-188, 199-200, 214, 226, 230, 234, 238, 242, 246, 251, 256, 261, 266, 271, 275, 279, 283, 295-300, 304-367, 381-384, 396-412, 426-455, 472-486, 498-502, 514-521, 533-538, 551-553, 570-603, 622, 637-648, 661-682, 707-722, 737-752, 767-782, 797-812, 827-842, 854-861, 873-880, 895-908, 923-939, 954-970, 985-1001
sparkless/functions/core/__init__.py                                     6      0   100%
sparkless/functions/core/column.py                                     455    368    19%   39, 43, 53, 57, 61, 65, 69, 73, 77, 81, 85, 89, 93, 97, 102, 108, 113, 119, 125, 131, 135, 139, 143, 147, 151, 155, 159, 163, 189-195, 199, 203, 209-210, 216, 220, 224, 239, 243, 247, 251, 255, 259, 263, 267, 285, 306, 331-345, 368-377, 382-384, 389, 393-395, 399, 418-434, 438, 442-445, 462-464, 468-470, 474-476, 480-482, 490, 498-508, 516-526, 534-544, 552-562, 570-580, 588-598, 607, 635-675, 685-889, 898-899, 907-957, 962, 967-1014, 1023, 1027-1040, 1044, 1048-1050
sparkless/functions/core/expressions.py                                109     74    32%   28-32, 55, 73, 91-94, 108-115, 127-129, 141-143, 155-157, 169-171, 188-228, 240-247, 259-266, 278-285, 297-304, 320-323
sparkless/functions/core/lambda_parser.py                              146    126    14%   58-134, 142-148, 167-175, 189-247, 260-274, 285-298, 309-314, 327-332, 359-361, 369, 377, 381-385
sparkless/functions/core/literals.py                                   131     87    34%   38-54, 65-76, 81, 86, 99-101, 108-110, 117-119, 123-125, 129-131, 135-137, 141-143, 147-149, 153-155, 159-161, 165-167, 171-173, 177-179, 183-185, 189-191, 195-197, 201-203, 207-209, 213, 217, 227-229, 233-240, 244-246, 250-252, 256-258, 262-268, 272-274, 278-280, 284-286, 302, 306-308, 312-314, 318-320
sparkless/functions/core/operations.py                                 106     67    37%   27-29, 33-35, 39-41, 45-47, 51-53, 57-59, 63-65, 69, 73, 77-79, 83-85, 89-91, 95-97, 101-103, 107-109, 122, 126, 130, 134, 145, 150-154, 158, 162, 176-189, 199, 203, 211, 219-221, 225-227, 235-237
sparkless/functions/core/sql_expr_parser.py                            294    294     0%   9-525
sparkless/functions/crypto.py                                           48     39    19%   49-71, 91-113, 133-155
sparkless/functions/datetime.py                                        474    333    30%   49-52, 69-74, 86-89, 98, 107, 119-125, 137-143, 155-161, 173-179, 188-189, 205-218, 231-237, 262-301, 327-366, 390-429, 455-500, 520-549, 564-580, 595-601, 616-622, 637-646, 658-664, 676-682, 694-700, 712-718, 730-736, 748-754, 772-794, 813-844, 856-860, 872-876, 888-892, 904-908, 920-924, 936-942, 954-960, 972-978, 990-994, 1006-1010, 1022-1026, 1039-1048, 1063-1074, 1087-1093, 1106-1112, 1125-1134, 1149-1159, 1179-1191, 1211-1222, 1231-1234, 1248-1253, 1262-1273, 1283-1294, 1317-1320, 1340-1343, 1368-1372, 1393-1396, 1422-1468, 1492-1497, 1518-1537, 1557-1560, 1576-1579, 1600-1603, 1622-1627, 1646-1647, 1664-1665, 1685-1693, 1711-1727, 1745-1761
sparkless/functions/functions.py                                      1547    688    56%   67-77, 89-92, 107, 117, 133-136, 142-162, 168-187, 193-212, 218-238, 244, 249, 254, 259, 264, 269, 274, 279, 286, 300, 307, 312, 317, 324, 329, 334, 341, 346, 351, 356, 361, 366, 371, 376, 381, 394-396, 401, 408, 415, 422, 427, 432, 437, 442, 447, 452, 457, 462, 467, 472, 483, 490, 497, 502, 507, 512, 523, 533, 543, 548, 555, 564, 569, 574, 581, 588, 593, 598, 605, 610, 615, 622, 627, 632, 637, 642, 647, 652, 657, 664, 669, 674, 680, 685, 690, 695, 700, 705, 710, 721, 726, 731, 736, 741, 748, 755, 760, 765, 770, 775, 780, 785, 790, 795, 800, 805, 810, 817, 822, 827, 832, 837, 842, 847, 852, 857, 862, 867, 872, 877, 882, 887, 892, 902, 907, 912, 917, 922, 927, 932, 937, 942, 948, 953, 958, 963, 968, 975, 980, 985, 990, 996, 1002, 1007, 1012, 1018, 1023, 1028, 1033, 1038, 1045, 1052, 1059, 1064, 1077, 1082, 1087, 1092, 1097, 1104, 1109, 1114, 1119, 1128, 1133, 1138, 1143, 1148, 1155, 1162, 1167, 1172, 1182-1183, 1192-1193, 1202-1205, 1212, 1219, 1224, 1231, 1238, 1243, 1248, 1253, 1258, 1263, 1269, 1274, 1279, 1284, 1293-1296, 1301, 1306, 1311, 1316, 1321, 1326, 1331, 1336, 1341, 1346, 1364-1420, 1425, 1430, 1435, 1442, 1447, 1452, 1457, 1466, 1471, 1476, 1483, 1491, 1496, 1501, 1506, 1511, 1516, 1521, 1533-1538, 1549, 1556, 1563, 1569-1571, 1579-1586, 1595-1597, 1607-1617, 1626-1634, 1643-1651, 1667-1674, 1690-1697, 1706-1713, 1722-1729, 1738-1745, 1754-1761, 1770-1777, 1786-1793, 1798-1803, 1809, 1814, 1819, 1824, 1829, 1834, 1841, 1848, 1855, 1860, 1865, 1873, 1880, 1887, 1894, 1904, 1913, 1919, 1924, 1929, 1934, 1939, 1946, 1951, 1956, 1963, 1968, 1973, 1978, 1983, 1988, 1993, 2000, 2005, 2010, 2015, 2020, 2029, 2034, 2040, 2045, 2050, 2055, 2062, 2068, 2073, 2078, 2085, 2092, 2099, 2108, 2114-2138, 2152-2158, 2169, 2174, 2179, 2185, 2190, 2195, 2200, 2205, 2210, 2215, 2220, 2225, 2233, 2242-2243, 2248, 2253, 2259, 2264, 2269, 2279, 2285, 2290, 2297, 2302, 2307, 2316-2318, 2326-2328, 2336-2338, 2346-2348, 2353-2355, 2360-2362, 2369-2371, 2378-2380, 2387-2389, 2396-2398, 2404, 2409, 2414, 2419, 2424, 2429, 2434, 2439, 2444, 2449, 2454, 2464-2466, 2471-2473, 2478-2480, 2485-2487, 2492-2494, 2503-2505, 2510-2512, 2517-2519, 2525-2527, 2532-2534, 2539-2541, 2546-2548, 2553-2555, 2561-2563, 2568-2570, 2575-2577, 2582-2584, 2589-2591, 2596-2598, 2603-2605, 2634-2687, 2716-2741, 2765-2772, 2789-2791, 2797, 2804, 2809, 2814, 2821, 2828, 2833, 2842, 2847, 2854, 2861, 2866, 2871, 2879, 2884, 2891, 2898, 2905, 2913, 2920, 2927, 2933, 2938, 2945, 2950, 2962, 2975, 2989, 3004, 3016, 3023, 3030, 3037, 3044, 3049, 3054, 3059, 3064, 3069, 3074, 3080, 3085, 3090, 3097, 3102, 3107, 3112, 3117, 3122, 3130, 3135, 3140, 3149, 3155, 3160, 3165, 3170, 3175, 3196-3213
sparkless/functions/json_csv.py                                         41     41     0%   8-161
sparkless/functions/map.py                                              99     70    29%   51-54, 69-72, 87-90, 107-115, 138-143, 167-218, 239-242, 262-265, 288-294, 320-326, 352-358, 387-396, 419-426
sparkless/functions/math.py                                            320    207    35%   50-54, 66-70, 82-86, 99-105, 117-121, 133, 145-149, 161-165, 177-182, 204-231, 246-249, 264-267, 284-287, 304-307, 322-334, 349, 361-365, 377-381, 393-397, 409-414, 426-439, 451-461, 476-477, 489-490, 505-506, 518-519, 531-532, 544-545, 567-574, 586-587, 599-600, 612-613, 625-626, 638-639, 651-652, 664-665, 677-679, 696-698, 715-716, 729-730, 745-748, 768-792, 809-810, 823-824, 836-837, 849-850, 859-862, 871-874, 886-887, 902-907, 921-926, 941-960, 972, 990-1003, 1026-1067
sparkless/functions/metadata.py                                         33     33     0%   8-109
sparkless/functions/ordering.py                                         28     28     0%   7-93
sparkless/functions/pandas_types.py                                      6      0   100%
sparkless/functions/string.py                                          547    372    32%   49-53, 65-69, 81-85, 97-103, 115-121, 133-137, 149-153, 165-169, 184-196, 209-218, 231-237, 250-256, 268-274, 287-296, 309-318, 331-337, 350-356, 370-379, 395, 411-420, 435-450, 462-468, 480-484, 496, 508, 521-534, 550-561, 578-590, 606-615, 628-641, 654-667, 683-693, 705-709, 721-725, 737-741, 760-769, 791-803, 818-822, 838-844, 859-863, 878-882, 900-903, 923-926, 941-944, 960-971, 1003-1006, 1030-1033, 1054-1057, 1075-1078, 1099-1102, 1124-1127, 1146-1149, 1169-1174, 1202-1223, 1235-1238, 1250-1253, 1265-1268, 1280-1299, 1316-1334, 1352-1355, 1370-1373, 1391-1394, 1414-1417, 1432-1435, 1451-1457, 1474-1477, 1492-1495, 1508-1514, 1529-1540, 1553-1562, 1575-1584, 1601-1610, 1627-1636, 1649-1655, 1673-1684, 1698-1710, 1725-1731, 1746-1752, 1764-1768, 1781-1792, 1807, 1832-1854, 1872-1879, 1897-1902, 1918-1927, 1939-1940
sparkless/functions/udf.py                                              51     41    20%   41-46, 57-58, 70-90, 119-121, 133-155
sparkless/functions/window_execution.py                                651    602     8%   34-85, 89, 100-101, 115, 129, 143, 157, 171, 185, 201-203, 219, 235-237, 253-255, 269, 283, 297, 311, 325, 339, 350, 361, 377, 390-428, 432-471, 478-553, 557-607, 613-629, 633-694, 698-742, 746-790, 794-845, 849-916, 920-981, 987-1024, 1028-1087, 1092, 1101-1146, 1150-1230, 1234-1260, 1268-1304, 1308-1372, 1376-1420, 1424-1468
sparkless/functions/xml.py                                              65     39    40%   25-28, 50-64, 84-87, 107-110, 131-134, 155-158, 179-182, 203-206, 227-230, 251-254, 275-278
sparkless/optimizer/__init__.py                                          3      3     0%   8-17
sparkless/optimizer/optimization_rules.py                              174    174     0%   7-376
sparkless/optimizer/query_optimizer.py                                 256    256     0%   8-518
sparkless/performance_simulation.py                                     91     91     0%   29-329
sparkless/session/__init__.py                                            4      0   100%
sparkless/session/catalog.py                                           258    208    19%   39, 43, 47, 60-61, 65, 69, 111, 119, 130-134, 142, 150, 170, 173, 177-180, 187-190, 212, 215, 225, 230-233, 250-318, 335-352, 374, 386-414, 430-450, 462-487, 498-519, 528, 537, 546, 566-575, 605-661, 666, 683-716
sparkless/session/config/__init__.py                                     2      0   100%
sparkless/session/config/configuration.py                               54     22    59%   77, 85-86, 94, 102, 110, 118-119, 130, 141, 146, 150, 188, 199-200, 211-212, 224-225, 236-237, 245
sparkless/session/context.py                                            36     10    72%   47, 51, 55, 92, 101, 110, 119, 123, 127, 131
sparkless/session/core/__init__.py                                       4      0   100%
sparkless/session/core/builder.py                                       30     18    40%   31, 42-43, 54, 68-72, 81-99
sparkless/session/core/session.py                                      227     81    64%   12-13, 122, 174-177, 182, 187, 202, 237-239, 249-254, 258-260, 273, 277, 287, 293, 306-308, 350, 368, 380, 391, 410-411, 414, 421-436, 444-446, 468, 472, 474, 479-482, 509-510, 525, 530-536, 544, 553-555, 575-580, 589, 597, 604, 611, 621, 628, 633-643, 652, 661-662, 667
sparkless/session/performance_tracker.py                                39     19    51%   57, 87-109, 117
sparkless/session/services/__init__.py                                   6      0   100%
sparkless/session/services/dataframe_factory.py                        216    176    19%   14-15, 73-78, 87-99, 102-103, 106, 119, 131-170, 176-193, 197-199, 204, 206, 211, 217-336, 341-364, 384-416, 428, 440, 447, 465-472, 490-506, 517-587
sparkless/session/services/lifecycle_manager.py                         21     10    52%   32-34, 42-43, 55-59
sparkless/session/services/mocking_coordinator.py                       32     21    34%   39-57, 69-72, 84-86, 102-104, 109
sparkless/session/services/protocols.py                                 20      0   100%
sparkless/session/services/sql_parameter_binder.py                      29     25    14%   28-51, 62-74
sparkless/session/session.py                                             0      0   100%
sparkless/session/sql/__init__.py                                        5      0   100%
sparkless/session/sql/executor.py                                     1280   1206     6%   76-90, 121, 123, 128-150, 170-1285, 1296-1348, 1383-1478, 1510, 1512, 1516-1523, 1544-1665, 1676-1705, 1720-1880, 1894-1988, 2002-2030, 2042-2088, 2100-2407, 2423-2644, 2667-2751, 2764-2779, 2806-2838, 2860-2867, 2899-2908, 2935-3001
sparkless/session/sql/optimizer.py                                      65     48    26%   41-43, 47, 51, 68, 85-100, 111-114, 127, 142, 155, 168, 181, 192-234, 247-260
sparkless/session/sql/parser.py                                        496    419    16%   45, 49, 263, 271-272, 287, 289, 291, 293, 295, 297, 302-315, 342, 344, 349-358, 387-687, 719-833, 881-882, 892, 906-993, 1010-1069, 1085-1106, 1121-1141, 1160-1275, 1291-1323, 1335-1338, 1349-1374
sparkless/session/sql/validation.py                                     85     72    15%   42, 105-126, 137-166, 177-188, 199-216, 227-235, 246-265, 276-279, 292, 306-311, 322-323
sparkless/spark_types.py                                               390    227    42%   60-74, 109-111, 115, 119-123, 127-151, 165, 211, 226, 241, 256, 271, 279-281, 285, 330-346, 350, 354, 362-364, 368, 372, 384, 396, 408, 420, 432, 439-440, 443, 450-451, 454, 466, 475-477, 480, 489-491, 494, 503-505, 508, 531, 539-544, 570-575, 585-590, 593, 602, 605-606, 610-613, 625-633, 654-656, 660, 664-665, 669, 673, 685, 698, 704-713, 718-733, 738-739, 746, 791, 802-805, 813-827, 831-859, 863-867, 871-878, 882-889, 893, 897-922, 934-957, 961-968, 974-976, 980-984, 990-991
sparkless/sql/__init__.py                                               10      0   100%
sparkless/sql/functions.py                                              30     13    57%   58-74, 84-92
sparkless/sql/types.py                                                   2      0   100%
sparkless/sql/utils.py                                                   7      0   100%
sparkless/storage/__init__.py                                           10      0   100%
sparkless/storage/backends/__init__.py                                   0      0   100%
sparkless/storage/backends/file.py                                     199    137    31%   26-34, 39, 44, 49, 53-56, 64-69, 77-78, 87-101, 112-119, 127, 135-138, 142, 146, 150, 154-155, 159-161, 174-177, 188-191, 202-203, 211-216, 224-232, 238, 242, 246, 250, 254, 258, 262-264, 268, 272, 276, 288-291, 299-300, 311, 320-327, 335, 347-349, 364-367, 376-377, 389-393, 408-413, 428-430, 442-448, 460, 470-481, 492-501, 505-509, 515-520, 527
sparkless/storage/backends/memory.py                                   133     60    55%   34, 44, 54, 57-59, 72-77, 93, 97, 101, 105, 109-110, 114-115, 160-161, 169, 208-209, 217, 230, 247, 258-259, 290-295, 310-312, 330, 342, 352-370, 381-390, 402-406, 418-423, 430
sparkless/storage/manager.py                                           135     86    36%   40-45, 58, 70, 82, 94-98, 108, 119, 128-130, 138, 142-149, 153-159, 171, 186, 195, 208, 223, 237, 249, 258, 269, 283, 295, 303, 312, 325, 336, 344-349, 353-358, 366-378, 382, 390-400, 411-432
sparkless/storage/models.py                                             67      1    99%   77
sparkless/storage/serialization/__init__.py                              0      0   100%
sparkless/storage/serialization/csv.py                                  46     32    30%   23-30, 42-47, 57-62, 76-92, 104-120
sparkless/storage/serialization/json.py                                 39     25    36%   23-24, 36-41, 51-63, 75-90, 102-118
sparkless/utils/profiling.py                                            96     43    55%   47-48, 58, 61, 71-76, 85, 88-90, 94, 99, 102, 108, 118-135, 158-172, 182, 188
sparkless/window.py                                                     67     46    31%   57-60, 77-96, 113-132, 147-151, 166-170, 174-189, 219, 229, 234, 239
--------------------------------------------------------------------------------------------------
TOTAL                                                                32723  26907    18%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml
=========================== short test summary info ============================
FAILED tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_existing_table - AssertionError: Table should have 1 row after append, got 2. This verifies fix for issue #114.
assert 2 == 1
FAILED tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_new_table - AssertionError: New table created by append should have 1 row, got 2
assert 2 == 1
FAILED tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_multiple_append_operations - AssertionError: After 1 appends, table should have 2 rows, got 4
assert 4 == 2
 +  where 4 = count()
 +    where count = DataFrame[4 rows, 2 columns].count
FAILED tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_active_session - assert 4 == 2
 +  where 4 = count()
 +    where count = DataFrame[4 rows, 2 columns].count
FAILED tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_multiple_sessions - assert 10 == 2
 +  where 10 = count()
 +    where count = DataFrame[10 rows, 2 columns].count
FAILED tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_storage_manager_detached_write_visible_to_session - assert 10 == 2
 +  where 10 = count()
 +    where count = DataFrame[10 rows, 2 columns].count
FAILED tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_pipeline_logs_like_write_visible_immediately - AssertionError: pipeline_logs table should return all appended rows
assert 20 == 4
 +  where 20 = count()
 +    where count = DataFrame[20 rows, 3 columns].count
ERROR tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_existing_table - AttributeError: 'RobinStorageManager' object has no attribute 'db_path'
ERROR tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_to_new_table - AttributeError: 'RobinStorageManager' object has no attribute 'db_path'
ERROR tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_multiple_append_operations - AttributeError: 'RobinStorageManager' object has no attribute 'db_path'
ERROR tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_active_session - AttributeError: 'RobinStorageManager' object has no attribute 'db_path'
ERROR tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_parquet_format_append_detached_df_visible_to_multiple_sessions - AttributeError: 'RobinStorageManager' object has no attribute 'db_path'
ERROR tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_storage_manager_detached_write_visible_to_session - AttributeError: 'RobinStorageManager' object has no attribute 'db_path'
ERROR tests/parity/dataframe/test_parquet_format_table_append.py::TestParquetFormatTableAppend::test_pipeline_logs_like_write_visible_immediately - AttributeError: 'RobinStorageManager' object has no attribute 'db_path'
========================= 7 failed, 7 errors in 6.81s ==========================
